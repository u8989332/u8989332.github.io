<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="GeekCodeParadise">
<meta property="og:url" content="http://example.com/page/21/index.html">
<meta property="og:site_name" content="GeekCodeParadise">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="LiJyu Gao">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/21/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>GeekCodeParadise</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">GeekCodeParadise</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2015/10/05/hive-1-2-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiJyu Gao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GeekCodeParadise">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2015/10/05/hive-1-2-1/" class="post-title-link" itemprop="url">Hive 1.2.1 安裝</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2015-10-05 23:19:00" itemprop="dateCreated datePublished" datetime="2015-10-05T23:19:00+08:00">2015-10-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-11-19 15:47:25" itemprop="dateModified" datetime="2023-11-19T15:47:25+08:00">2023-11-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Hadoop/" itemprop="url" rel="index"><span itemprop="name">Hadoop</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Hive/" itemprop="url" rel="index"><span itemprop="name">Hive</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Java/" itemprop="url" rel="index"><span itemprop="name">Java</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Hive 1.2.1 安裝</p>
<h4 id="1-Hive簡介"><a href="#1-Hive簡介" class="headerlink" title="1.Hive簡介"></a>1.Hive簡介</h4><p>Hive是一種透過Hadoop的MapReduce機制，使資料以Database的形式存在HDFS，並能用HiveQL作類似SQL語言來查詢資料，使得大型的資料能有快速的被查詢。Hive原先是由Facebook開發，後來貢獻給Apache，近幾年許多人不斷更改、維護，功能更加完善，目前最新版為1.2.1版。另外Hive存資料schema的格式是放在metastore，預設是用in-memory的Derby，但Derby只支援一個User的操作Hive，需要另外用MySQL來存放metastore。本篇會介紹如何安裝Hive、用Hive存資料與透過Java的JDBC存取Hive。</p>
<h4 id="2-安裝Hive"><a href="#2-安裝Hive" class="headerlink" title="2.安裝Hive"></a>2.安裝Hive</h4><p>在安裝Hive前，先確認環境已經安裝Hadoop與MySQL。若沒安裝過MySQL可以參考這篇文章。<a target="_blank" rel="noopener" href="https://geekcodeparadise.com/2015/10/ubuntu-install-uninstall-mysql/">[My</a><a target="_blank" rel="noopener" href="https://geekcodeparadise.com/2015/10/ubuntu-install-uninstall-mysql/">S</a><a target="_blank" rel="noopener" href="https://geekcodeparadise.com/2015/10/ubuntu-install-uninstall-mysql/">QL]在ubuntu移除與安裝MySQL</a><br>　　Hive是安裝在需要使用的server，所以是安裝在Master（hadoop01）。首先用wget下載1.2.1版的Hive：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo wget http://apache.stu.edu.tw/hive/hive-1.2.1/apache-hive-1.2.1-bin.tar.gz</span><br></pre></td></tr></table></figure>

<p>　　下載後將其解壓縮：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf apache-hive-1.2.1-bin.tar.gz</span><br></pre></td></tr></table></figure>

<p>　　在.bashrc增加Hive的環境變數：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#編輯.bashrc檔</span></span><br><span class="line">sudo vim ~/.bashrc</span><br><span class="line"><span class="comment">#export Hive環境變數</span></span><br><span class="line"><span class="built_in">export</span> HIVE_HOME=/home/test/apache-hive-1.2.1-bin</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$HIVE_HOME</span>/bin:<span class="variable">$HIVE_HOME</span>/conf:<span class="variable">$PATH</span></span><br><span class="line"><span class="comment">#存檔後記得source</span></span><br><span class="line"><span class="built_in">source</span> ~/.bashrc</span><br></pre></td></tr></table></figure>

<p>　　建立Hive所需的HDFS目錄與更改權限：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#/tmp目錄可能已經存在了，若已存在也須改權限</span></span><br><span class="line">hadoop fs -<span class="built_in">mkdir</span> /tmp　　<span class="comment">#主要用在存放一些Hive執行過程的臨時資料</span></span><br><span class="line">hadoop fs -<span class="built_in">mkdir</span> /user/hive/warehouse <span class="comment">#Hive進行管理的資料目錄，例如table都會存在這</span></span><br><span class="line">hadoop fs -<span class="built_in">chmod</span> 777 /tmp</span><br><span class="line">hadoop fs -<span class="built_in">chmod</span> 777 /user/hive/warehouse</span><br></pre></td></tr></table></figure>

<p>　　安裝libmysql-java，用JDBC時需要用到：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install libmysql-java</span><br></pre></td></tr></table></figure>

<p>　　安裝完成後，將&#x2F;usr&#x2F;share&#x2F;java&#x2F;mysql-connector-java-5.1.28.jar複製到hive&#x2F;lib目錄下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo <span class="built_in">cp</span> /usr/share/java/mysql-connector-java-5.1.28.jar ~/apache-hive-1.2.1-bin/lib</span><br></pre></td></tr></table></figure>

<p>　　接著啟動MySQL，建立一個專屬Hive的帳號，啟動方式如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql -u root -p<span class="comment">#接著會要求您輸入root的密碼，輸入完後會進入到mysql &gt; 的command模式</span></span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mysql &gt; create database hive; <span class="comment">#建立一個hive的database</span></span><br><span class="line">mysql&gt; grant all on *.* to<span class="string">&#x27;hive&#x27;</span>@<span class="string">&#x27;%&#x27;</span> identified by <span class="string">&#x27;hive&#x27;</span>; <span class="comment">#建立一個MySQL使用者，帳號跟密碼都是hive，且用%代表在任何hostname都可登入</span></span><br><span class="line">mysql&gt; flush privileges; <span class="comment">#更新User清單</span></span><br><span class="line">mysql&gt; <span class="built_in">exit</span>; <span class="comment">#結束mysql</span></span><br></pre></td></tr></table></figure>

<p>　　修改MySQL的參數設定檔案my.cnf：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /etc/mysql/my.cnf</span><br><span class="line"><span class="comment">#將bind-address = 127.0.0.1這行用#註解，不要讓MySQL綁定local</span></span><br><span class="line"><span class="comment">#bind-address = 127.0.0.1</span></span><br></pre></td></tr></table></figure>

<p>　　在Hive的目錄裡建立一個新目錄：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> -p /home/test/apache-hive-1.2.1-bin/iotmp <span class="comment">#Hive config會需要用到</span></span><br><span class="line"><span class="built_in">chmod</span> 777 /home/test/apache-hive-1.2.1-bin/iotmp</span><br></pre></td></tr></table></figure>

<p>　　進入到Hive的目錄，將apache-hive-1.2.1-bin&#x2F;conf下的hive-default.xml.template改成hive-site.xml：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cp</span> hive-default.xml.template hive-site.xml</span><br></pre></td></tr></table></figure>

<p>　　更改hive-site.xml的設定：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vim hive-site.xml</span><br></pre></td></tr></table></figure>

<p>　　另外更改conf的hive-env.sh.template：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cp</span> hive-env.sh.template hive-env.sh</span><br><span class="line">sudo vim hive-env.sh</span><br><span class="line"><span class="comment">#在這檔案內容最下方增加這4項:</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_HEAPSIZE=1024</span><br><span class="line">HADOOP_HOME=/home/test/hadoop-2.7.1</span><br><span class="line"><span class="built_in">export</span> HIVE_CONF_DIR=/home/test/apache-hive-1.2.1-bin/conf</span><br><span class="line"><span class="built_in">export</span> HIVE_AUX_JARS_PATH=/home/test/apache-hive-1.2.1-bin/lib</span><br></pre></td></tr></table></figure>

<p>　　設定檔都存好後，接著啟動metastore：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive --service metastore &amp; <span class="comment">#後面一定要加個&amp;符號，才可以再按下enter跳過啟動的訊息，之前一直誤會怎啟動卡住了。。。</span></span><br></pre></td></tr></table></figure>

<p>　　用jps -m查看，可以發現多了一個是metastore RunJar的process，代表metastore啟動成功。</p>
<h4 id="3-用Hive新增table"><a href="#3-用Hive新增table" class="headerlink" title="3.用Hive新增table"></a>3.用Hive新增table</h4><p>　　metastore啟動後，下指令hive，將會進入hive &gt; 的commnad模式。<br>　　首先新增一筆table叫做student：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create table student(<span class="built_in">id</span> int, name string) row format delimited fields terminated by <span class="string">&#x27;t&#x27;</span>;</span><br></pre></td></tr></table></figure>

<p>　　這一行建立table的意思是這table student是由int型態的id與string型態的name兩種屬性組成，而每一列資料（row format delimited）是預設跳行隔開，每一列資料彼此之間是用tab（terminated by ‘t’）隔開。<br>　　建好後用show tables可以查看table是否新增成功，如下圖：</p>
<p><a target="_blank" rel="noopener" href="http://geekcodeparadise.com/wp-content/uploads/2015/10/show2Btables253B-1.png"><img src="http://geekcodeparadise.com/wp-content/uploads/2015/10/show2Btables253B.png" alt="Hive 1.2.1 describe"></a></p>
<p>　　在使用describe student看該table的屬性，如下圖：</p>
<p><a target="_blank" rel="noopener" href="http://geekcodeparadise.com/wp-content/uploads/2015/10/decs2Btable-1.png"><img src="http://geekcodeparadise.com/wp-content/uploads/2015/10/decs2Btable.png"></a></p>
<p>　　在&#x2F;home&#x2F;test目錄下，建個要匯入student的資料：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sudo vim ~/StudentFile.txt</span><br><span class="line"><span class="comment">#資料內容如下，記得id與name是用tab隔開</span></span><br><span class="line">1 John</span><br><span class="line">2 Marry</span><br><span class="line">3 Frank</span><br><span class="line">4 Jessie</span><br></pre></td></tr></table></figure>

<p>　　再次進入hive command模式，將資料匯入：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; load data <span class="built_in">local</span> inpath<span class="string">&#x27;/home/test/StudentFile.txt&#x27;</span> overwrite into table student;</span><br></pre></td></tr></table></figure>

<p>　　用HiveQL語法查詢是否有資料匯入：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; <span class="keyword">select</span> <span class="built_in">id</span> from student;</span><br><span class="line">hive&gt; <span class="keyword">select</span> name from student;</span><br><span class="line">hive&gt; <span class="keyword">select</span> <span class="built_in">id</span> from student <span class="built_in">where</span> name=<span class="string">&#x27;Frank&#x27;</span>;</span><br></pre></td></tr></table></figure>

<p>　　這三筆查詢的結果如下圖，語法跟SQL非常像～</p>
<p><a target="_blank" rel="noopener" href="http://geekcodeparadise.com/wp-content/uploads/2015/10/select2Bid2Band2Bname-1.png"><img src="http://geekcodeparadise.com/wp-content/uploads/2015/10/select2Bid2Band2Bname.png" alt="Hive 1.2.1 select"></a></p>
<p><a target="_blank" rel="noopener" href="http://geekcodeparadise.com/wp-content/uploads/2015/10/SELECT2BFRANK-1.png"><img src="http://geekcodeparadise.com/wp-content/uploads/2015/10/SELECT2BFRANK.png" alt="Hive 1.2.1 select"></a></p>
<p>　　接著下count的語法，會發現有啟動MapReduce的資訊顯示：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; <span class="keyword">select</span> count(<span class="built_in">id</span>) from student <span class="built_in">where</span> name like <span class="string">&#x27;%a%&#x27;</span>;</span><br></pre></td></tr></table></figure>

<p>　　如下圖所示：</p>
<p><a target="_blank" rel="noopener" href="http://geekcodeparadise.com/wp-content/uploads/2015/10/count2Bmap1-1.png"><img src="http://geekcodeparadise.com/wp-content/uploads/2015/10/count2Bmap1.png" alt="Hive 1.2.1 select like"></a></p>
<p><a target="_blank" rel="noopener" href="http://geekcodeparadise.com/wp-content/uploads/2015/10/count2Bmap2-1.png"><img src="http://geekcodeparadise.com/wp-content/uploads/2015/10/count2Bmap2.png" alt="Hive 1.2.1 select result"></a></p>
<p>　　查詢有name有存在小寫a的資料有幾筆，其輸出的2筆為Frank和Marry（當時做紀錄打錯名字。。。懶的更正XD）。<br>　　當Hive有新增table後，可以用hive帳號登入MySQL，查詢TBLS這張表的內容，可看見有Hive新增的table名稱、建立時間等資訊，代表MySQL作為metastore的運作，如下圖所示：</p>
<p><a target="_blank" rel="noopener" href="http://geekcodeparadise.com/wp-content/uploads/2015/10/mysql2Bselect2Bfrom2Btbls-1.png"><img src="http://geekcodeparadise.com/wp-content/uploads/2015/10/mysql2Bselect2Bfrom2Btbls.png" alt="Hive 1.2.1 select *"></a></p>
<h4 id="4-用Java連Hive-JDBC"><a href="#4-用Java連Hive-JDBC" class="headerlink" title="4.用Java連Hive JDBC"></a>4.用Java連Hive JDBC</h4><p>　　之前已經複製mysql-connector-java-5.1.28.jar的library了，接著啟動hiveserver2，使Java程式能透過JDBC連到Hive：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive --service hiveserver2 &amp; <span class="comment">#一樣要用&amp;，啟動後一樣是建立新的RunJar的process</span></span><br></pre></td></tr></table></figure>

<p>　　接著我們在Windows的Eclipse建立一個新的Java class來測試JDBC，其程式碼如下：</p>
<p>　　基本上程式碼跟自行用hive&gt; 下HiveQL一樣，只差在用DriverManager連JDBC。而程式執行的結果如下圖：</p>
<p><a target="_blank" rel="noopener" href="http://geekcodeparadise.com/wp-content/uploads/2015/10/ECLIPSE2BRUN2BHIVE-1.png"><img src="http://geekcodeparadise.com/wp-content/uploads/2015/10/ECLIPSE2BRUN2BHIVE.png"></a></p>
<p>　　也可進入hive command驗證是否有新的table匯入。</p>
<h4 id="5-結論"><a href="#5-結論" class="headerlink" title="5.結論"></a>5.結論</h4><p>　　安裝Hive花我超多時間在survey資料，新版的安裝方式跟舊版天差地遠，官方文件又少得可憐，蒐集了各方好手的文獻，好不容易才run起這個傢伙。。。<br>　　另外透過上述的HiveQL count，發現用MapReduce在這少量資料是非常浪費時間，需要有用到大筆的資料才能顯現Hive的優勢～</p>
<h4 id="參考文獻"><a href="#參考文獻" class="headerlink" title="參考文獻"></a>參考文獻</h4><ol>
<li><a target="_blank" rel="noopener" href="http://1oscar.github.io/blog/2015/07/19/Dive%20into%20hive.html">段家公子blog：dive into hive</a></li>
<li><a target="_blank" rel="noopener" href="http://blog.csdn.net/x_i_y_u_e/article/details/46845609">x_i_y_u_e blog：Hadoop Hive安装，配置mysql元数据库</a> </li>
<li><a target="_blank" rel="noopener" href="http://lizhenliang.blog.51cto.com/7876557/1665891">zhenliang8 blog：基于Hadoop数据仓库Hive1.2部署及使用</a> </li>
<li><a target="_blank" rel="noopener" href="http://yanliu.org/2015/08/13/Hadoop%E9%9B%86%E7%BE%A4%E4%B9%8BHive%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/">Yan’s blog：Hadoop集群之Hive安装配置</a> </li>
<li> <a target="_blank" rel="noopener" href="http://f.dataguru.cn/thread-459379-1-1.html">jf_32635344論壇文章： [原创] Hive启动时，遇到java.net.URISyntaxException: Relative path in absolute URI</a> </li>
<li><a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/HiveClient">Hive官方JDBC Client程式碼</a></li>
<li><a target="_blank" rel="noopener" href="http://stackoverflow.com/questions/18128966/where-is-the-mysql-jdbc-jar-file-in-ubuntu">Stackoverflow：Where is the MySQL JDBC jar file in Ubuntu?</a></li>
</ol>
<p> </p>
<h4 id="新增-修改日記"><a href="#新增-修改日記" class="headerlink" title="新增&#x2F;修改日記"></a>新增&#x2F;修改日記</h4><p>2015&#x2F;10&#x2F;6：</p>
<ol>
<li>新增MySQL my.cnf檔案內的參數設定，少了這一步驟會無法啟動Hive。</li>
</ol>
<p>2015&#x2F;10&#x2F;9：</p>
<ol>
<li>新增用MySQL查看Hive註冊過的table資訊。</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2015/10/01/hdfs-java-api/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiJyu Gao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GeekCodeParadise">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2015/10/01/hdfs-java-api/" class="post-title-link" itemprop="url">HDFS Java API</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2015-10-01 07:44:00" itemprop="dateCreated datePublished" datetime="2015-10-01T07:44:00+08:00">2015-10-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-11-19 15:47:25" itemprop="dateModified" datetime="2023-11-19T15:47:25+08:00">2023-11-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Hadoop/" itemprop="url" rel="index"><span itemprop="name">Hadoop</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/HDFS/" itemprop="url" rel="index"><span itemprop="name">HDFS</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Java/" itemprop="url" rel="index"><span itemprop="name">Java</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>HDFS Java API</p>
<h4 id="1-HDFS的Java-API"><a href="#1-HDFS的Java-API" class="headerlink" title="1.HDFS的Java API"></a>1.HDFS的Java API</h4><p>HDFS提供Java的API，可以用Java開發對HDFS做存取功能的程式。本篇開發環境一樣是Java EE Eclipse，以下會介紹HDFS常使用的API。</p>
<h5 id="1-1-繼承-TestCase-class的-java檔"><a href="#1-1-繼承-TestCase-class的-java檔" class="headerlink" title="1.1 繼承 TestCase class的.java檔"></a>1.1 繼承 TestCase class的.java檔</h5><p>如果您已經熟悉JUnit這項功能，可直接跳過本節。由於HDFS的API很多重複性的函式／類別要呼叫，如果每做一個功能就要建一個.java來測試短短幾行程式碼，個人覺得蠻占空間的。而外界有提供JUnit API（已內建於Java EE Eclipse Maven Project），這API可以在同一份.java檔針對某個函式做執行。首先要建立個MyHDFSControl.java，在檔案上方import：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> junit.framework.TestCase;</span><br></pre></td></tr></table></figure>

<p>　　之後在class名稱後面用extends繼承TestCase：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MyHDFSControl</span> <span class="keyword">extends</span> <span class="title class_">TestCase</span> &#123;　　<span class="comment">//code....&#125;</span></span><br></pre></td></tr></table></figure>

<p>　　如此一來該類別的每個函式都可以獨自執行。</p>
<h5 id="1-2-讀取HDFS檔案的內容"><a href="#1-2-讀取HDFS檔案的內容" class="headerlink" title="1.2 讀取HDFS檔案的內容"></a>1.2 讀取HDFS檔案的內容</h5><p>　　首先先貼這簡短的程式碼，在依序介紹每行的作用：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testRead</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span><br><span class="line">&#123;</span><br><span class="line"> <span class="comment">//hadoop01 HDFS的Uri</span></span><br><span class="line"> <span class="type">String</span> <span class="variable">basicUri</span> <span class="operator">=</span> <span class="string">&quot;webhdfs://hadoop01&quot;</span>;</span><br><span class="line"> <span class="comment">//之前做Wordcount的計算結果Uri</span></span><br><span class="line"> <span class="type">String</span> <span class="variable">uri</span> <span class="operator">=</span> <span class="string">&quot;webhdfs://hadoop01/output01/part-r-00000&quot;</span>;</span><br><span class="line"> <span class="comment">//建立HDFS操作，都一定要呼叫環境變數類別Configuration</span></span><br><span class="line"> <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line"> <span class="comment">//建立HDFS的類別FileSystem，參數帶入hadoop01的Uri與Configuration</span></span><br><span class="line"> <span class="type">FileSystem</span> <span class="variable">hdfs</span> <span class="operator">=</span> FileSystem.get(URI.create(basicUri),conf);</span><br><span class="line"></span><br><span class="line"> <span class="comment">//建立HDFS路徑的類別Path，參數帶入要做存取檔案的Uri</span></span><br><span class="line"> <span class="type">Path</span> <span class="variable">path</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Path</span>(uri);</span><br><span class="line">  </span><br><span class="line"> <span class="comment">/*建立HDFS讀取檔案的類別FSDataInputStream,</span></span><br><span class="line"><span class="comment">    由FileSystem的open函式傳遞path的stream*/</span></span><br><span class="line"> <span class="type">FSDataInputStream</span> <span class="variable">inStream</span> <span class="operator">=</span> hdfs.open(path);</span><br><span class="line"> <span class="comment">/*透過hadoop的IO API IOUtils，參數分別是(1)inStream讀取的資料 (2)stdout(console)輸出，</span></span><br><span class="line"><span class="comment"> 　(3)每次的存取buffer size是預設的4096 Bytes，(4)設false代表存取完不close stream */</span></span><br><span class="line"> IOUtils.copyBytes(inStream, System.out, <span class="number">4096</span>,<span class="literal">false</span>);</span><br><span class="line"> <span class="comment">//關掉stream</span></span><br><span class="line"> IOUtils.closeStream(inStream);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>　　首先介紹Uri與FileSystem：用Java做開發時，都要先透過FileSystem建立連線，使FileSystem能知道要存取HDFS的位置。而連線的Uri是帶入host hadoop01(master)的位置：webhdfs:&#x2F;&#x2F;hadoop01。<br>　　若有讀者曾開發過，會看出比較特別的地方是我Uri使用webhdfs:&#x2F;&#x2F; protocol，而不是hdfs:&#x2F;&#x2F;。因為webhdfs是可以兼容各種版本的HDFS，不用擔心版本的差異會導致無法存取HDFS。<br>　　接著Path是帶入要存取的HDFS檔案路徑，再透過FileSystem.open的方式，將該Path的FSInputStream指定給inStream變數，如此一來建立了對此檔案的檔案輸入指標。<br>　　Hadoop的IO類別的IOUtils可以對stream做讀取&#x2F;寫入，因此使用IOUtils將inStream讀到的資料，輸出到stdout(console)，最後再關閉inStream。<br> 　　執行程式的方式與結果如下：</p>
<p><a target="_blank" rel="noopener" href="http://geekcodeparadise.com/wp-content/uploads/2015/10/right2Bclick2Brun2Bjunit-1.png"><img src="http://geekcodeparadise.com/wp-content/uploads/2015/10/right2Bclick2Brun2Bjunit.png" alt="HDFS Java API running"></a></p>
<p>對函式的名稱testRead右鍵-&gt;Run As-&gt;JUnit Test，或者在右邊的Outline也是一樣做法</p>
<p><a target="_blank" rel="noopener" href="http://geekcodeparadise.com/wp-content/uploads/2015/10/read2Boutput-1.png"><img src="http://geekcodeparadise.com/wp-content/uploads/2015/10/read2Boutput.png" alt="HDFS Java API run unit testing"></a></p>
<p>Console的輸出結果是之前做Wordcount的結果</p>
<p>　　大部分HDFS操作的流程： 建立環境Configuration -&gt; 建立FileSystem -&gt; 與host Uri連線 -&gt; 建立檔案的Path -&gt; 用FileSystem對Path做存取的操作。<br>　　以下各種程式碼的大同小異，會針對較特殊的地方再做補充說明。</p>
<h5 id="1-3-HDFS目錄下的資料屬性"><a href="#1-3-HDFS目錄下的資料屬性" class="headerlink" title="1.3 HDFS目錄下的資料屬性"></a>1.3 HDFS目錄下的資料屬性</h5><p>程式碼：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testList</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span><br><span class="line">&#123;</span><br><span class="line"> <span class="type">String</span> <span class="variable">basicUri</span> <span class="operator">=</span> <span class="string">&quot;webhdfs://hadoop01/&quot;</span>;</span><br><span class="line"> <span class="type">String</span> <span class="variable">uri</span> <span class="operator">=</span> <span class="string">&quot;webhdfs://hadoop01/input01&quot;</span>;</span><br><span class="line"> <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line"> <span class="type">FileSystem</span> <span class="variable">hdfs</span> <span class="operator">=</span> FileSystem.get(URI.create(basicUri),conf);</span><br><span class="line"> <span class="type">Path</span> <span class="variable">path</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Path</span>(uri);</span><br><span class="line"> <span class="comment">//用listStatus函式，讀取input01目錄下的資料</span></span><br><span class="line"> FileStatus[] fileStatus = hdfs.listStatus(path);</span><br><span class="line"> <span class="comment">//用迴圈執行判斷每個資料是file或directory</span></span><br><span class="line"> <span class="keyword">for</span> (FileStatus fs : fileStatus)</span><br><span class="line"> &#123;</span><br><span class="line">  <span class="type">Path</span> <span class="variable">p</span> <span class="operator">=</span> fs.getPath();</span><br><span class="line">  <span class="type">String</span> <span class="variable">info</span> <span class="operator">=</span> fs.isDir() ? <span class="string">&quot;Directory&quot;</span> : <span class="string">&quot;File&quot;</span>;</span><br><span class="line">  System.out.println(info + <span class="string">&quot; : &quot;</span> + p);</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>　　用listStatus的函式，可以將一個目錄下所有資料的屬性，再透過迴圈讀取每個檔案的內容，本節使用Path的函式isDir判斷這檔案是否為directory。Path的函式還有很多種，例如該檔案的大小、owner是誰等。</p>
<h5 id="1-4-建立HDFS目錄"><a href="#1-4-建立HDFS目錄" class="headerlink" title="1.4 建立HDFS目錄"></a>1.4 建立HDFS目錄</h5><p>程式碼：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testCreateDir</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line"> <span class="keyword">try</span></span><br><span class="line"> &#123;</span><br><span class="line">  <span class="type">String</span> <span class="variable">basicUri</span> <span class="operator">=</span> <span class="string">&quot;webhdfs://hadoop01/&quot;</span>;</span><br><span class="line"><span class="comment">//建立testNewDir/testDir01目錄</span></span><br><span class="line">  <span class="type">String</span> <span class="variable">uri</span> <span class="operator">=</span> <span class="string">&quot;webhdfs://hadoop01/testNewDir/testDir01&quot;</span>;</span><br><span class="line">  <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">  <span class="type">FileSystem</span> <span class="variable">hdfs</span> <span class="operator">=</span> FileSystem.get(URI.create(basicUri),conf);</span><br><span class="line">   </span><br><span class="line">  <span class="type">Path</span> <span class="variable">path</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Path</span>(uri);</span><br><span class="line">  <span class="type">boolean</span> <span class="variable">isSuccess</span> <span class="operator">=</span> hdfs.mkdirs(path);</span><br><span class="line">  <span class="type">String</span> <span class="variable">info</span> <span class="operator">=</span> isSuccess ? <span class="string">&quot;success&quot;</span> : <span class="string">&quot;fail&quot;</span>;</span><br><span class="line">  System.out.println(<span class="string">&quot;Create directory [&quot;</span> + path + <span class="string">&quot;]&quot;</span> + info);</span><br><span class="line"> &#125;</span><br><span class="line"> <span class="keyword">catch</span>(Exception e)</span><br><span class="line"> &#123;</span><br><span class="line">  e.printStackTrace();</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>　　對於資料的寫入有個較謹慎的問題：權限！原先我的HDFS沒有testNewDir目錄，也沒有testDir01目錄，所以若是直接執行這段程式，會出現Permission denied的訊息：</p>
<p>Permission denied: user&#x3D;GAO, access&#x3D;WRITE, inode&#x3D;”&#x2F;testNewDir&#x2F;testDir01”:test:supergroup:drwxr-xr-x</p>
<p>　　解決的辦法有2種：<br>　　1.Server用指令先建立個testNewDir目錄，並用chmod改變這目錄的權限：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -<span class="built_in">mkdir</span> /testNewDir</span><br><span class="line">hadoop fs -<span class="built_in">chmod</span> 777 /testNewDir</span><br></pre></td></tr></table></figure>

<p>　　執行完這程式碼後，再執行testCreateDir()程式就可以正常在testNewDir目錄下建立testDir01目錄(或寫入檔案）</p>
<p>　　2.更改hdfs-site.xml的內容，將dfs.permissions設置為false。但這方法不建議，會變成所有人都能隨意存取，有很多安全性的問題。</p>
<p>　　3.在Eclipse裡設定Java執行的環境變數，在Eclipse上方點Run -&gt; Run Configurations… -&gt; Environment -&gt; New -&gt; 新增一個NAME為HADOOP_USER_NAME ，Value為test（代表server的user），設定好按下Apply。如下圖所示：</p>
<p><a target="_blank" rel="noopener" href="http://geekcodeparadise.com/wp-content/uploads/2015/10/RUN2BCONIG2BHADOOP2BUSER-1.png"><img src="http://geekcodeparadise.com/wp-content/uploads/2015/10/RUN2BCONIG2BHADOOP2BUSER.png" alt="HDFS Java API environment"></a></p>
<h5 id="1-5-複製local端的資料到HDFS"><a href="#1-5-複製local端的資料到HDFS" class="headerlink" title="1.5 複製local端的資料到HDFS"></a>1.5 複製local端的資料到HDFS</h5><p>　　程式碼：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testPut</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line"> <span class="keyword">try</span></span><br><span class="line"> &#123;</span><br><span class="line">  </span><br><span class="line">  <span class="type">String</span> <span class="variable">basicUri</span> <span class="operator">=</span> <span class="string">&quot;webhdfs://hadoop01/&quot;</span>;</span><br><span class="line">  <span class="type">String</span> <span class="variable">srcUri</span> <span class="operator">=</span> <span class="string">&quot;c:/testfile.txt&quot;</span>;</span><br><span class="line">  <span class="type">String</span> <span class="variable">destUri</span> <span class="operator">=</span> <span class="string">&quot;webhdfs://hadoop01/testNewDir&quot;</span>;</span><br><span class="line">  <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">  <span class="type">FileSystem</span> <span class="variable">hdfs</span> <span class="operator">=</span> FileSystem.get(URI.create(basicUri),conf);</span><br><span class="line">  </span><br><span class="line">  <span class="type">Path</span> <span class="variable">srcPath</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Path</span>(srcUri);</span><br><span class="line">  <span class="type">Path</span> <span class="variable">destPath</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Path</span>(destUri);</span><br><span class="line">  <span class="comment">/* 將local端的資料放到HDFS上 */</span></span><br><span class="line">  hdfs.copyFromLocalFile(srcPath, destPath);</span><br><span class="line">  </span><br><span class="line"> &#125;</span><br><span class="line"> <span class="keyword">catch</span>(Exception e)</span><br><span class="line"> &#123;</span><br><span class="line">  e.printStackTrace();</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>　　使用copyFromLocalFile函式可以將local的資料寫到HDFS上。</p>
<h5 id="1-6-在HDFS寫入檔案"><a href="#1-6-在HDFS寫入檔案" class="headerlink" title="1.6 在HDFS寫入檔案"></a>1.6 在HDFS寫入檔案</h5><p>　　程式碼：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testCreateFile</span><span class="params">()</span></span><br><span class="line"> &#123;</span><br><span class="line">  <span class="keyword">try</span></span><br><span class="line">  &#123;</span><br><span class="line">   <span class="type">String</span> <span class="variable">basicUri</span> <span class="operator">=</span> <span class="string">&quot;webhdfs://hadoop01/&quot;</span>;</span><br><span class="line">   <span class="type">String</span> <span class="variable">uri</span> <span class="operator">=</span> <span class="string">&quot;webhdfs://hadoop01/testNewDir/test1.txt&quot;</span>;</span><br><span class="line">   <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">   <span class="type">FileSystem</span> <span class="variable">hdfs</span> <span class="operator">=</span> FileSystem.get(URI.create(basicUri),conf);</span><br><span class="line">   <span class="type">Path</span> <span class="variable">path</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Path</span>(uri);</span><br><span class="line">   <span class="comment">/* 在HDFS寫入FILE要用FSDataOutputStream */</span></span><br><span class="line">   <span class="type">FSDataOutputStream</span> <span class="variable">fsDataOutputStream</span> <span class="operator">=</span> hdfs.create(path);</span><br><span class="line">   fsDataOutputStream.write(<span class="string">&quot;This is test&quot;</span>.getBytes());</span><br><span class="line">   IOUtils.closeStream(fsDataOutputStream);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">catch</span>(Exception e)</span><br><span class="line">  &#123;</span><br><span class="line">   e.printStackTrace();</span><br><span class="line">  &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<p>　　在HDFS寫入檔案使用FSDataOutputStream類別，透過hdfs的.create()回傳stream（FSInputStream是用open，有差別）。寫入資料的方式是用FsDataOutputStream.write()，這邊要注意的是若要寫入字串，盡可能使用getBytes()來寫入，若是用writeUTF()或writeChars()等函式寫入字串，寫出來的檔案會多一些字元，這屬於編碼的問題，詳細的原因待探討。</p>
<h5 id="1-7-更改HDFS檔案的名稱"><a href="#1-7-更改HDFS檔案的名稱" class="headerlink" title="1.7 更改HDFS檔案的名稱"></a>1.7 更改HDFS檔案的名稱</h5><p>程式碼：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testRename</span><span class="params">()</span></span><br><span class="line"> &#123;</span><br><span class="line">  <span class="keyword">try</span></span><br><span class="line">  &#123;</span><br><span class="line">   <span class="type">String</span> <span class="variable">basicUri</span> <span class="operator">=</span> <span class="string">&quot;webhdfs://hadoop01/&quot;</span>;</span><br><span class="line">   <span class="type">String</span> <span class="variable">srcuri</span> <span class="operator">=</span> <span class="string">&quot;webhdfs://hadoop01/testNewDir/test1.txt&quot;</span>;</span><br><span class="line">   <span class="type">String</span> <span class="variable">dsturi</span> <span class="operator">=</span> <span class="string">&quot;webhdfs://hadoop01/testNewDir/test2.txt&quot;</span>;</span><br><span class="line">   <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">   </span><br><span class="line">   <span class="type">FileSystem</span> <span class="variable">hdfs</span> <span class="operator">=</span> FileSystem.get(URI.create(basicUri),conf);</span><br><span class="line">   </span><br><span class="line">   <span class="type">Path</span> <span class="variable">srcpath</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Path</span>(srcuri);</span><br><span class="line">   <span class="type">Path</span> <span class="variable">dstpath</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Path</span>(dsturi);</span><br><span class="line">   <span class="type">boolean</span> <span class="variable">flag</span> <span class="operator">=</span> hdfs.rename(srcpath,dstpath);</span><br><span class="line">   System.out.println(flag);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">catch</span>(Exception e)</span><br><span class="line">  &#123;</span><br><span class="line">   e.printStackTrace();</span><br><span class="line">  &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<h5 id="1-8-刪除HDFS檔案-目錄"><a href="#1-8-刪除HDFS檔案-目錄" class="headerlink" title="1.8 刪除HDFS檔案&#x2F;目錄"></a>1.8 刪除HDFS檔案&#x2F;目錄</h5><p>程式碼：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testDeleteFile</span><span class="params">()</span></span><br><span class="line"> &#123;</span><br><span class="line">  <span class="keyword">try</span></span><br><span class="line">  &#123;</span><br><span class="line">   <span class="type">String</span> <span class="variable">basicUri</span> <span class="operator">=</span> <span class="string">&quot;webhdfs://hadoop01/&quot;</span>;</span><br><span class="line">   <span class="type">String</span> <span class="variable">uri</span> <span class="operator">=</span> <span class="string">&quot;webhdfs://hadoop01/testNewDir&quot;</span>;</span><br><span class="line">   <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">   <span class="type">FileSystem</span> <span class="variable">hdfs</span> <span class="operator">=</span> FileSystem.get(URI.create(basicUri),conf);</span><br><span class="line">   <span class="type">Path</span> <span class="variable">path</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Path</span>(uri);</span><br><span class="line">   <span class="comment">/* 第二個參數:是否遞迴方式刪除, 若刪除對象是目錄,則目錄也會跟著被刪 */</span></span><br><span class="line">   <span class="type">boolean</span> <span class="variable">flag</span> <span class="operator">=</span> hdfs.delete(path,<span class="literal">true</span>);</span><br><span class="line">   System.out.println(flag);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">catch</span>(Exception e)</span><br><span class="line">  &#123;</span><br><span class="line">   e.printStackTrace();</span><br><span class="line">  &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<h5 id="1-9-列出HDFS的各個Datanode-Hostname"><a href="#1-9-列出HDFS的各個Datanode-Hostname" class="headerlink" title="1.9 列出HDFS的各個Datanode Hostname"></a>1.9 列出HDFS的各個Datanode Hostname</h5><p>程式碼：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">testCluster</span><span class="params">()</span></span><br><span class="line"> &#123;</span><br><span class="line">  <span class="keyword">try</span></span><br><span class="line">  &#123;</span><br><span class="line">   <span class="comment">// 要用hdfs protocol,且port要用9000</span></span><br><span class="line">   <span class="type">String</span> <span class="variable">basicUri</span> <span class="operator">=</span> <span class="string">&quot;hdfs://hadoop01:9000/&quot;</span>;</span><br><span class="line">   <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">   <span class="type">FileSystem</span> <span class="variable">hdfs</span> <span class="operator">=</span> FileSystem.get(URI.create(basicUri),conf);</span><br><span class="line">   </span><br><span class="line">   <span class="comment">//將FileSystem轉型給DistributedFileSystem</span></span><br><span class="line">   <span class="type">DistributedFileSystem</span> <span class="variable">distributedFileSystem</span> <span class="operator">=</span> (DistributedFileSystem) hdfs;</span><br><span class="line">   <span class="comment">//回傳各個datanode的資訊</span></span><br><span class="line">   DatanodeInfo[] datanodeInfos = distributedFileSystem.getDataNodeStats();</span><br><span class="line">   <span class="keyword">for</span>(DatanodeInfo dn : datanodeInfos)</span><br><span class="line">   &#123;</span><br><span class="line">    <span class="comment">//回傳每個datanode的hostname</span></span><br><span class="line">    <span class="type">String</span> <span class="variable">hostname</span> <span class="operator">=</span> dn.getHostName();</span><br><span class="line">    System.out.println(hostname);</span><br><span class="line">   &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">catch</span>(Exception e)</span><br><span class="line">  &#123;</span><br><span class="line">   e.printStackTrace();</span><br><span class="line">  &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<p>　　這邊較特別的是，要用DistributedFileSystem必須用hdfs的protocol，且port要用9000，用原本的webhdfs會無法將FileSystem cast給DistributedFileSystem。另外執行Java的環境變數要設置HADOOP_USER_NAME為test。以本人的程式執行會輸出hadoop01、hadoop02與hadoop03。</p>
<h4 id="2-結論"><a href="#2-結論" class="headerlink" title="2.結論"></a>2.結論</h4><p>　　透過HDFS的Java API，可以想像各種網路雲端空間的實作方式是不是就是這些API呢？只差在GUI、資料安全性、硬體等（其實差很多XD）。預計之後寫Hive與HBase的文章。</p>
<h4 id="參考文獻"><a href="#參考文獻" class="headerlink" title="參考文獻"></a>參考文獻</h4><ol>
<li>陸嘉恒 Hadoop實戰 第2版 第9章 HDFS詳解</li>
<li><a target="_blank" rel="noopener" href="http://www.huqiwen.com/2013/07/18/hdfs-permission-denied/">IT人生录BLOG：HDFS客户端的权限错误：Permission denied</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2015/09/26/eclipse-mapreduce-wordcount/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiJyu Gao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GeekCodeParadise">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2015/09/26/eclipse-mapreduce-wordcount/" class="post-title-link" itemprop="url">Eclipse MapReduce Wordcount</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2015-09-26 23:08:00" itemprop="dateCreated datePublished" datetime="2015-09-26T23:08:00+08:00">2015-09-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-11-19 15:47:25" itemprop="dateModified" datetime="2023-11-19T15:47:25+08:00">2023-11-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Hadoop/" itemprop="url" rel="index"><span itemprop="name">Hadoop</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Eclipse/" itemprop="url" rel="index"><span itemprop="name">Eclipse</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Java/" itemprop="url" rel="index"><span itemprop="name">Java</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/MapReduce/" itemprop="url" rel="index"><span itemprop="name">MapReduce</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Eclipse MapReduce Wordcount</p>
<h4 id="1-開發環境"><a href="#1-開發環境" class="headerlink" title="1.開發環境"></a>1.開發環境</h4><p>在Windows（本篇為Windows 7）開發Hadoop程式需要的軟體工具清單：</p>
<ul>
<li>(1) Oracle JDK，本篇使用1.8.0_60版， <a target="_blank" rel="noopener" href="http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html">官方連結點我下載</a></li>
<li>(2) Eclipse，寫JAVA程式的極好IDE，本篇使用 Eclipse IDE for Java EE Developers Mars Release 4.5.0版本，<a target="_blank" rel="noopener" href="https://eclipse.org/downloads/download.php?file=/technology/epp/downloads/release/mars/R/eclipse-jee-mars-R-win32-x86_64.zip">官方連結點我下載</a></li>
</ul>
<p> </p>
<p>　　安裝完JDK後，記得在Windows的環境變數新增JAVA_HOME變數，並指定到您的JAVA安裝目錄，以本篇是安裝在C:Program FilesJavajre1.8.0_60</p>
<h4 id="2-Eclipse新增Hadoop專案"><a href="#2-Eclipse新增Hadoop專案" class="headerlink" title="2.Eclipse新增Hadoop專案"></a>2.Eclipse新增Hadoop專案</h4><h5 id="2-1-Maven的簡介"><a href="#2-1-Maven的簡介" class="headerlink" title="2.1 Maven的簡介"></a>2.1 Maven的簡介</h5><p>Apache Maven是現在開發JAVA程式很常用的管理專案套件，在較新的Eclipse JavaEE版有內建。主要功能有2項：</p>
<ol>
<li>可以自動從特定的library repository下載開發程式的所需jar檔，甚至有相依性的jar檔也都會一起下載，比起傳統要自己從每個網站東抓西抓檔案還來的有效率。</li>
<li>專案區分main與test兩種程式碼管理，test有使用JUnit做單元測試，在開發較複雜的系統可以用JUnit測完該程式的功能，再放置main裡面。</li>
</ol>
<p>本篇使用Maven來建立Hadoop程式，其他的管理專案套件有Ivy、Grape、Gradle、Buildr、SBT、Leiningen等，有興趣者可以使用其他的。</p>
<h5 id="2-2-新增Maven-Project"><a href="#2-2-新增Maven-Project" class="headerlink" title="2.2 新增Maven Project"></a>2.2 新增Maven Project</h5><p>在Eclipse的左上角點選File -&gt; New -&gt; Maven Projec -&gt; 選擇quickstart的archetype -&gt; 設定Group Id(類似於 package 的功能，作為專案的群組識別名稱)、Artifact Id(專案名稱)。如下面三圖所示：</p>
<p><a target="_blank" rel="noopener" href="http://geekcodeparadise.com/wp-content/uploads/2015/09/NEW2BMAVEN2BPROJECT-1.png"><img src="http://geekcodeparadise.com/wp-content/uploads/2015/09/NEW2BMAVEN2BPROJECT.png" alt="Eclipse MapReduce Wordcount maven project"></a></p>
<p><a target="_blank" rel="noopener" href="http://geekcodeparadise.com/wp-content/uploads/2015/09/MAVEN2BARCHTYPE-1.png"><img src="http://geekcodeparadise.com/wp-content/uploads/2015/09/MAVEN2BARCHTYPE.png" alt="Eclipse MapReduce Wordcount maven project"></a></p>
<p><img src="http://geekcodeparadise.com/wp-content/uploads/2015/09/MAVEN2BPROJECT2BSET.png" alt="Eclipse MapReduce Wordcount project detail"></p>
<p> <a target="_blank" rel="noopener" href="http://geekcodeparadise.com/wp-content/uploads/2015/09/MAVEN2BPROJECT2BSET-1.png"></a>  　　建立好專案後，在專案檔案瀏覽的介面，可看見有src&#x2F;main&#x2F;java、src&#x2F;test&#x2F;java、Maven Dependency與pom.xml等目錄與檔案。如下圖所示：</p>
<p><a target="_blank" rel="noopener" href="http://geekcodeparadise.com/wp-content/uploads/2015/09/project2Bcreate2Band2Bchoose2Bpom-1.png"><img src="http://geekcodeparadise.com/wp-content/uploads/2015/09/project2Bcreate2Band2Bchoose2Bpom.png" alt="Eclipse MapReduce Wordcount pom.xml"></a></p>
<p>點選pom.xml，預設在<dependencies>的標籤內只有junit套件，為了要開發Hadoop的程式，要另外加以下的套件：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">groupid</span>&gt;</span>jdk.tools<span class="tag">&lt;/<span class="name">groupid</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">artifactid</span>&gt;</span>jdk.tools<span class="tag">&lt;/<span class="name">artifactid</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">scope</span>&gt;</span>system<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">systempath</span>&gt;</span>$&#123;JAVA_HOME&#125;/lib/tools.jar<span class="tag">&lt;/<span class="name">systempath</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">groupid</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupid</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">artifactid</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactid</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">groupid</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupid</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">artifactid</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactid</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"> </span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">groupid</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupid</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">artifactid</span>&gt;</span>hadoop-hdfs<span class="tag">&lt;/<span class="name">artifactid</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>　　加了這些套件設定，儲存pom.xml，此時Eclipse右下角會顯示正在連結repository下載所需套件的進度，如下圖所示：</p>
<p><a target="_blank" rel="noopener" href="http://geekcodeparadise.com/wp-content/uploads/2015/09/pomxml2Badd2Bhadoop-1.png"><img src="http://geekcodeparadise.com/wp-content/uploads/2015/09/pomxml2Badd2Bhadoop.png" alt="Eclipse MapReduce Wordcount update pom.xml"></a></p>
<p> 　　下載完畢，在專案檔案瀏覽介面點選Maven dependencies，可以看見所有的套件已經匯入該專案中，這麼一來可以開發基本的Hadop的程式。如下圖所示：</p>
<p><a target="_blank" rel="noopener" href="http://geekcodeparadise.com/wp-content/uploads/2015/09/maven2Bdependence2Blist-1.png"><img src="http://geekcodeparadise.com/wp-content/uploads/2015/09/maven2Bdependence2Blist.png" alt="Eclipse MapReduce Wordcount libraries"></a></p>
<h5 id="2-3-開發Hadoop-JAVA程式-MyWordcount"><a href="#2-3-開發Hadoop-JAVA程式-MyWordcount" class="headerlink" title="2.3 開發Hadoop JAVA程式 - MyWordcount"></a>2.3 開發Hadoop JAVA程式 - MyWordcount</h5><p> 　　在src&#x2F;main&#x2F;java的package MyHadoopProject.Hadoop_2_7_1右鍵點選New -&gt; Class，取名較MyWordcount的class，如下圖所示：</p>
<p><a target="_blank" rel="noopener" href="http://geekcodeparadise.com/wp-content/uploads/2015/09/mywordcount2Bclass-1.png"><img src="http://geekcodeparadise.com/wp-content/uploads/2015/09/mywordcount2Bclass.png" alt="Eclipse MapReduce Wordcount create program"></a></p>
<p>　　在寫程式前，先講解MapReduce程式的基本架構，其一共區分三種區塊：</p>
<p>　　Mapper區：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">void</span> Class Mapper ...&#123; <span class="comment">//Map程式碼&#125;</span></span><br></pre></td></tr></table></figure>

<p>　　Reducer區：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">void</span> Class Reducer...&#123; <span class="comment">//Reduce程式碼&#125;</span></span><br></pre></td></tr></table></figure>

<p>　　Driver區：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">main()&#123; <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>(); <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Job</span>(conf,<span class="string">&quot;MyWordcount&quot;</span>); job.setJarByClass(thisMainClass.class); job.setMapperClass(Mapper.class); job.setReducerClass(Reducer.class); FileInputFormat.addInputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">0</span>])); FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">1</span>])); <span class="comment">//其他配置參數程式碼 job.waitForCompletion(true);&#125;</span></span><br></pre></td></tr></table></figure>

<p>　接著根據此架構，撰寫出本程式的完整程式碼：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> MyHadoopProject.Hadoop_2_7_1;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.StringTokenizer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MyWordcountWithPrint</span> &#123;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">public</span> <span class="title function_">MyWordcountWithPrint</span><span class="params">()</span> &#123;</span><br><span class="line">  <span class="comment">// TODO Auto-generated constructor stub</span></span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException &#123;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">/* 初始化 */</span></span><br><span class="line">  <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">  </span><br><span class="line">  <span class="comment">/* 建立MapReduce Job, 該job的名稱為MyWordcount */</span></span><br><span class="line">  <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Job</span>(conf,<span class="string">&quot;MyWordcountPrint&quot;</span>);</span><br><span class="line">  </span><br><span class="line">  <span class="comment">/* 啟動job的jar class 為MyWordcount */</span></span><br><span class="line">  job.setJarByClass(MyWordcountWithPrint.class);</span><br><span class="line">  <span class="comment">/* 啟動job的map class 為MyMapper */</span></span><br><span class="line">  job.setMapperClass(MyMapper.class);</span><br><span class="line">  <span class="comment">/* 啟動job的reduce class 為MyReducer */</span></span><br><span class="line">  job.setReducerClass(MyReducer.class);</span><br><span class="line">  </span><br><span class="line">  <span class="comment">/* 輸入資料的HDFS路徑 */</span></span><br><span class="line">  FileInputFormat.addInputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/input02&quot;</span>));</span><br><span class="line">  <span class="comment">/* 輸出資料的HDFS路徑 */</span></span><br><span class="line">  FileOutputFormat.setOutputPath(job,  <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;/output03&quot;</span>));</span><br><span class="line">  </span><br><span class="line">  <span class="comment">/* 輸出Key的型別 */</span></span><br><span class="line">  job.setOutputKeyClass(Text.class);</span><br><span class="line">  <span class="comment">/* 輸出Value的型別 */</span></span><br><span class="line">  job.setOutputValueClass(IntWritable.class);</span><br><span class="line">  </span><br><span class="line">  <span class="comment">/* 啟動Job並回傳是否成功執行完畢 */</span></span><br><span class="line">  <span class="type">boolean</span> <span class="variable">isSuccess</span> <span class="operator">=</span> job.waitForCompletion(<span class="literal">true</span>);</span><br><span class="line">  </span><br><span class="line">  System.exit(isSuccess ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line"> &#125;</span><br><span class="line"> </span><br><span class="line"> <span class="comment">/* Mapper adapter: input key(LongWritable) , input value(Text) ,</span></span><br><span class="line"><span class="comment">  *  output key(Text) , output value(IntWritable) */</span></span><br><span class="line"> </span><br><span class="line"> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">MyMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span></span><br><span class="line">&lt;LongWritable, Text, Text , IntWritable&gt;&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> <span class="type">IntWritable</span> <span class="variable">one</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>(<span class="number">1</span>);</span><br><span class="line">  <span class="keyword">private</span> <span class="type">Text</span> <span class="variable">word</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException</span><br><span class="line">  &#123;</span><br><span class="line">   <span class="comment">/* 將每一行的字串存到lineValue */</span></span><br><span class="line">   System.out.println(<span class="string">&quot;map content:&quot;</span>+key.get() +  <span class="string">&quot;and&quot;</span> + value.toString());</span><br><span class="line">   <span class="type">String</span> <span class="variable">lineValue</span> <span class="operator">=</span> value.toString();</span><br><span class="line">   </span><br><span class="line">   <span class="comment">/* 用StringTokenizer分割有空白、跳行等字元 */</span></span><br><span class="line">   <span class="type">StringTokenizer</span> <span class="variable">stk</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StringTokenizer</span>(lineValue);</span><br><span class="line">   </span><br><span class="line">   <span class="comment">/* 將每個切割的字串存到wordValue, 再將wordValue設為Reduce的Key,</span></span><br><span class="line"><span class="comment">             * value設為整數1 (one) */</span></span><br><span class="line">   <span class="keyword">while</span>(stk.hasMoreTokens())</span><br><span class="line">   &#123;</span><br><span class="line">    <span class="type">String</span> <span class="variable">wordValue</span> <span class="operator">=</span> stk.nextToken();</span><br><span class="line">    word.set(wordValue);</span><br><span class="line">    context.write(word, one);</span><br><span class="line">   &#125;</span><br><span class="line">  &#125;</span><br><span class="line"> &#125;</span><br><span class="line"> </span><br><span class="line"> <span class="comment">/* Reducer adapter: input key(Text) , input values(IntWritable) ,</span></span><br><span class="line"><span class="comment">  *  output key(Text) , output value(IntWritable) */</span></span><br><span class="line"> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">MyReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span></span><br><span class="line">    &lt;Text, IntWritable, Text , IntWritable&gt;&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="type">IntWritable</span> <span class="variable">result</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>();</span><br><span class="line">  <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable</span></span><br><span class="line"><span class="params">        &lt;intwritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException</span><br><span class="line">  &#123;</span><br><span class="line">   <span class="comment">/* 累加該單字的數量 */</span></span><br><span class="line">   <span class="type">int</span> <span class="variable">sum</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">   </span><br><span class="line">   <span class="comment">/* 在Iterable變數values用迴圈方式,將每個值(整數1)取出並累加 */</span></span><br><span class="line">   <span class="keyword">for</span>(IntWritable value: values)</span><br><span class="line">   &#123;</span><br><span class="line">    sum += value.get();</span><br><span class="line">   &#125;</span><br><span class="line">   </span><br><span class="line">   <span class="comment">/* 將累加的結果存到result */</span> </span><br><span class="line">   result.set(sum);</span><br><span class="line">   </span><br><span class="line">   <span class="comment">/* 輸出計算的結果 */</span></span><br><span class="line">   context.write(key,result);</span><br><span class="line">  &#125;</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>　　剛看完程式碼也許還不懂其意思，在此講解各程式碼區塊的運作：</p>
<ol>
<li>Mapper：mapper程式讀取輸入資料的接口，其中參數有4項：input key(LongWritable) , input value(Text) , output key(Text) 與 output value(IntWritable)。input key(LongWritable)是在一個檔案內每一行的起始位置（長整數型態）；input value(Text)是在一個檔案內每一行的字串；output key(Text) 是輸出的字串資料key，output value(IntWritable)是輸出的整數資料value。</li>
</ol>
<p>　　用個例子來說明會較清楚假設有個檔案內容為下 :</p>
<p>hello<br>world<br>hello<br>bug</p>
<p>那麼input key則會是0、6、12與18(包含跳行n)，input value是hello、world、hello與bug。StringTokenizer是切字串的類別，以此例子每行只有一個word，所以token只會有1個字。當得到一個token wordValue後，用Text型別的變數word，將token給設置：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">word.set(wordValue);</span><br></pre></td></tr></table></figure>

<p>　　接著要輸出到reduce階段的函式是使用context.write()，其參數前後分別是key、value，而value是整數1（&#x3D; 變數one &#x3D; IntWritable(1)），代表該單字出現１次：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">context.write(word, one);</span><br></pre></td></tr></table></figure>

<ol>
<li><p>Reducer：reducer是將mapper的輸出作為輸入，其中參數有4項：Reducer adapter: input key(Text) , input values(IntWritable) , output key(Text)與output value(IntWritable)。 　　input value(Text)與input values(IntWritable)要對應mapper的output key(Text)與output value(IntWritable)。而輸出的output key(Text) , output value(IntWritable) 則是該檔案的單字字串與出現的頻率。在以上面例子繼續講解，透過mapper傳過來的資料，其input key與input values會是：</p>
<p>(hello,(1,1))<br>(world,(1))<br>(bug,(1))</p>
<p>　　在MyReducer的protected void reduce函式裡，可以看見values是以iterable迭代器帶入，可以用迴圈來累加該key的出現次數。而要將整數值取出的方式是用.get()：</p>
</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sum += value.get();</span><br></pre></td></tr></table></figure>

<p>最後一樣是用context將output的key與value輸出，輸出的內容值將會是：</p>
<p>(hello,2)<br>(world,1)<br>(bug,1)</p>
<p>2. Driver：整個程式的啟動設定，包含設置主要有Configuration、Job與input&#x2F;output 資料HDFS路徑。Configuration目前只需new個物件及可；<br>　　Job首先set該MapReduce的名稱.set(conf,”job name”)；再來set MapReduce的class分別是main(MyWordcount.class)、mapper(MyMapper.class)與reducer(MyReducer.class)；</p>
<p>　　最後set輸出的key與value型態分為Text與IntWritable。input&#x2F;output的HDFS路徑是用FileInputFormat.addInputPath(job, new Path(“&#x2F;input01”))與 FileOutputFormat.setOutputPath(job, new Path(“&#x2F;output02”))，本篇是直接指定這兩個路徑，其中&#x2F;input01是上次用Hadoop wordcount example的輸入目錄。</p>
<p>　　整體來看，其實MapReduce的輸入輸出都是 (key,value) pair，再更核心部分會用shuffle、combine、sort等處理，這部分的演算法未來會介紹。</p>
<h5 id="2-4-Export-jar檔-與-Run-wordcount-job"><a href="#2-4-Export-jar檔-與-Run-wordcount-job" class="headerlink" title="2.4 Export jar檔 與 Run wordcount job"></a>2.4 Export jar檔 與 Run wordcount job</h5><p>code寫好後，點選在專案目錄的MyWordcount.java右鍵-&gt;Export-&gt;JAVA:Runnable JAR file，輸出名為MyWordcount.jar，如下方2圖所示：</p>
<p><img src="http://geekcodeparadise.com/wp-content/uploads/2015/09/export2Bjar.png" alt="jar"></p>
<p><img src="http://geekcodeparadise.com/wp-content/uploads/2015/09/export2Bjar2.png" alt="jar export"></p>
<p> 　　接著將jar檔用ftp的方式上傳到hadoop01的&#x2F;home&#x2F;test&#x2F;目錄下，使用Filezilla不再贅述。接著下指令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar MyWordcount.jar</span><br></pre></td></tr></table></figure>

<p>　　此時command視窗會顯示該MapReduce程式啟動的資訊，如上次的example一樣，如下圖所示：</p>
<p><img src="http://geekcodeparadise.com/wp-content/uploads/2015/09/run2Bjar.png" alt="result"></p>
<p>　　在port 8088的頁面中，也可以看見該job執行的狀況，如下圖所示：</p>
<p><img src="http://geekcodeparadise.com/wp-content/uploads/2015/09/run2Bjob.png" alt="job"></p>
<p> 　　job執行完後，開啟port 50070的頁面，在&#x2F;output02的目錄下有產生資料，接著打開來檢查，確實內容跟上次用example的結果是一樣的，代表自己的wordcount是正確的！如下圖所示：</p>
<p><img src="http://geekcodeparadise.com/wp-content/uploads/2015/09/OUTPUT022BDIR.png" alt="hdfs"></p>
<p><img src="http://geekcodeparadise.com/wp-content/uploads/2015/09/MYCOUNTRESULT.png" alt="result detail"></p>
<h4 id="3-結論"><a href="#3-結論" class="headerlink" title="3.結論"></a>3.結論</h4><p>懂了MapReduce的程式撰寫方式後，就能稱霸全世界，可以嘗試寫些比較複雜的資料處理程式。下篇內容會寫如何用JAVA存取HDFS資料。</p>
<h4 id="參考文獻"><a href="#參考文獻" class="headerlink" title="參考文獻"></a>參考文獻</h4><ol>
<li>Stackoverflow：<a target="_blank" rel="noopener" href="http://stackoverflow.com/questions/29195269/in-the-pom-xml-for-a-java-project-i-get-missing-artifact-jdk-toolsjdk-toolsja">In the pom.xml for a java project, I get missing artifact jdk.tools:jdk.tools:jar:1.6 error</a>，在用Maven時遇到的bug，需要帶入JDK 1.8版本的lib路徑</li>
<li><a target="_blank" rel="noopener" href="http://learngeb-ebook.readbook.tw/integration/maven.html">Gitbook readbook : Maven章節</a></li>
<li><a target="_blank" rel="noopener" href="http://learngeb-ebook.readbook.tw/integration/maven.html">Hadoop官方MapReduce tutorial</a></li>
</ol>
<h4 id="新增-修改日記"><a href="#新增-修改日記" class="headerlink" title="新增&#x2F;修改日記"></a>新增&#x2F;修改日記</h4><p>2015&#x2F;9&#x2F;29：</p>
<ol>
<li>移除Maven其中1項dependency: hadoop-core，這項不移除會導致一些function衝突</li>
</ol>
<p>Eclipse MapReduce Wordcount</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2015/09/20/create-hdfs-mapreduce-wordcount/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiJyu Gao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GeekCodeParadise">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2015/09/20/create-hdfs-mapreduce-wordcount/" class="post-title-link" itemprop="url">Create HDFS MapReduce Wordcount</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2015-09-20 21:00:00" itemprop="dateCreated datePublished" datetime="2015-09-20T21:00:00+08:00">2015-09-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-11-19 15:47:25" itemprop="dateModified" datetime="2023-11-19T15:47:25+08:00">2023-11-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Hadoop/" itemprop="url" rel="index"><span itemprop="name">Hadoop</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/HDFS/" itemprop="url" rel="index"><span itemprop="name">HDFS</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/MapReduce/" itemprop="url" rel="index"><span itemprop="name">MapReduce</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Create HDFS &amp; MapReduce Wordcount</p>
<h4 id="1-建立HDFS"><a href="#1-建立HDFS" class="headerlink" title="1.建立HDFS"></a>1.建立HDFS</h4><p>安裝好Hadoop Cluster後，接著要在HDFS放上資料，並執行Hadoop經典的程式範例 - Wordcount。其程式名稱如字面所示，是可以計算文字檔裡面詞彙的數量。</p>
<p>　　安裝的那一篇前言提到，HDFS(Hadoop Distributed File System)是分散式的檔案系統，要透過Hadoop做運算，都得從HDFS存取資料。</p>
<p>　　首先我們把local的資料搬到HDFS上，將hadoop&#x2F;etc&#x2F;hadoop&#x2F;目錄下各種參數設定的檔案搬過去，在hadoop01(slaves也行)要下此指令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop dfs -put ~/hadoop-2.7.1/etc/hadoop /input01</span><br></pre></td></tr></table></figure>

<p>　　先來分析這指令，</p>
<ol>
<li> dfs：要做HDFS的存取，都要用這參數，或者使用fs行，兩種功能是一樣的。</li>
<li>-put：要從server local的資料搬到HDFS上，要用此-put參數</li>
<li>src_dir1 src_dir2 …：-put後面接著的參數是local資料的路徑，其目錄可以不只1個，而本篇是只有用1個目錄：~&#x2F;hadoop-2.7.1&#x2F;etc&#x2F;hadoop。</li>
<li>des_dir：-put最後一個的參數則是HDFS的目錄，本篇是放在&#x2F;input01下。原先我的HDFS沒有&#x2F;input01這目錄，使用-put後，Hadoop會自動創立此目錄。</li>
</ol>
<p>更多的HDFS command指令請參考<a target="_blank" rel="noopener" href="http://hadoop.apache.org/docs/r1.2.1/file_system_shell.html">Hadoop Documentation File System Shell Guide</a> ，和Linux的檔案系統指令非常相像，若有熟悉使用Linux將會很快上手。</p>
<p>　　放上去之後，一起用web介面查看是否有上傳成功，其畫面如下：</p>
<p><a target="_blank" rel="noopener" href="http://geekcodeparadise.com/wp-content/uploads/2015/09/hdfs2Bweb2Binput01-1.png"><img src="http://geekcodeparadise.com/wp-content/uploads/2015/09/hdfs2Bweb2Binput01.png" alt="Create HDFS &amp; MapReduce Wordcount web"></a></p>
<p>　　畫面中顯示多了&#x2F;input01目錄，代表有成功的將local資料搬過去。該目錄中的每個檔案有超連結可點，若是檔案的話，會顯示其所在block資訊，包含block ID、檔案大小、放在哪個datanode上等資訊，且也可以下載。我點選capacity-scheduler.xml此檔案的連結，會如下圖所示：</p>
<p><a target="_blank" rel="noopener" href="http://geekcodeparadise.com/wp-content/uploads/2015/09/hdfs2Bfile2Binfo-1.png"><img src="http://geekcodeparadise.com/wp-content/uploads/2015/09/hdfs2Bfile2Binfo.png" alt="Create HDFS &amp; MapReduce Wordcount  capacity-scheduler.xml"></a></p>
<h4 id="2-執行MapReduce程式"><a href="#2-執行MapReduce程式" class="headerlink" title="2.執行MapReduce程式"></a>2.執行MapReduce程式</h4><h5 id="2-1-新增yarn-site-xml參數"><a href="#2-1-新增yarn-site-xml參數" class="headerlink" title="2.1 新增yarn-site.xml參數"></a>2.1 新增yarn-site.xml參數</h5><p>執行MapReduce時，可以在port 8088的頁面中瀏覽job執行紀錄(history log)，使開發者可以參考程式是否運作正常。為了讓history log可以正常瀏覽，先用stop-all.sh將所有service關閉，然後在Master與Slaves的yarn-site.xml裡面多加些參數。</p>
<p>　　yarn-site.xml新增的參數為：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 若沒加這參數，會無法在port 8088的History Tracking UI正常瀏覽 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h5 id="2-2-啟動MapReduce-JobHistory-Server"><a href="#2-2-啟動MapReduce-JobHistory-Server" class="headerlink" title="2.2 啟動MapReduce JobHistory Server"></a>2.2 啟動MapReduce JobHistory Server</h5><p>　　在執行MapReduce程式之前，先將JobHistory Server的service啟動，這service是可記錄每一個MapReduce的執行前後狀況，可用來做程式的Debug、效率分析等。<br>　　在全部的機器都要執行此指令啟動：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mr-jobhistory-daemon.sh start historyserver</span><br></pre></td></tr></table></figure>

<p>　　可透過jps指令檢查是否有開啟 JobHistory Server：</p>
<p><a target="_blank" rel="noopener" href="http://geekcodeparadise.com/wp-content/uploads/2015/09/JPS-JOBHISTORYSERVICE-1.png"><img src="http://geekcodeparadise.com/wp-content/uploads/2015/09/JPS-JOBHISTORYSERVICE.png" alt="Create HDFS MapReduce Wordcount jps"></a></p>
<p>　　因為目前還沒有任何程式執行過，所以在port 8088的網頁上仍不會有job紀錄。</p>
<h5 id="2-3-執行MapReduc程式-Wordcount"><a href="#2-3-執行MapReduc程式-Wordcount" class="headerlink" title="2.3 執行MapReduc程式 - Wordcount"></a>2.3 執行MapReduc程式 - Wordcount</h5><p>在HDFS上有了資料後，可以透過MapReduce對這些資料做運算。在我們的hadoop&#x2F;share&#x2F;hadoop&#x2F;mapreduce&#x2F;的目錄下，有一支jar檔案：hadoop-mapreduce-examples-2.7.1.jar，其提供許多的MapReduce範例讓開發者可以使用。將目錄切換後，試著下指令，查看有提供什麼範例：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar hadoop-mapreduce-examples-2.7.1.jar</span><br></pre></td></tr></table></figure>

<p>　　可看見列出一堆的參數名稱：</p>
<p><a target="_blank" rel="noopener" href="http://geekcodeparadise.com/wp-content/uploads/2015/09/jar2Bexample-1.png"><img src="http://geekcodeparadise.com/wp-content/uploads/2015/09/jar2Bexample.png" alt="Create HDFS MapReduce Wordcount 參數"></a></p>
<p>　　有對資料做sort、計算圓周率Pi、解數獨(sudoku)等範例，本篇以wordcount最常示範的程式來執行MapReduce。</p>
<p> 　　指令如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar hadoop-mapreduce-examples-2.7.1.jar wordcount /input01 /output01</span><br></pre></td></tr></table></figure>

<p>　　wordcount後面要帶的兩個參數分別為在HDFS的input目錄與output目錄，&#x2F;input01為第1節所建立的資料，而&#x2F;output01為wordcount計算完後的輸出目錄。<br>　　此時command畫面會列出一堆INFO資訊，其內容包啟動該程式的Job ID、input有多少數量、Map和Reduce的進度及執行完程式後，花費多少記憶體、CPU時間、讀寫檔案量等，如下面三張圖所示： 　</p>
<p><a target="_blank" rel="noopener" href="http://geekcodeparadise.com/wp-content/uploads/2015/09/WORDCOUNT2BRUN1-1.png"><img src="http://geekcodeparadise.com/wp-content/uploads/2015/09/WORDCOUNT2BRUN1.png" alt="Create HDFS MapReduce Wordcount process"></a></p>
<p><a target="_blank" rel="noopener" href="http://geekcodeparadise.com/wp-content/uploads/2015/09/WORDCOUNT2BRUN2-1.png"><img src="http://geekcodeparadise.com/wp-content/uploads/2015/09/WORDCOUNT2BRUN2.png" alt="Create HDFS MapReduce Wordcount process"></a></p>
<p><img src="http://geekcodeparadise.com/wp-content/uploads/2015/09/WORDCOUNT2BRUN3.png" alt="Create HDFS MapReduce Wordcount process"></p>
<p>　　當程式仍在執行時，可瀏覽port 8088網頁的RUNNING頁面查看job執行狀況，如下圖：</p>
<p><img src="http://geekcodeparadise.com/wp-content/uploads/2015/09/HADOOPRUNNIGN2BPAGE.png" alt="Create HDFS MapReduce Wordcount web"></p>
<p>　　最後執行完成後，該job會到FINISHED的頁面：</p>
<p><img src="http://geekcodeparadise.com/wp-content/uploads/2015/09/finish2Bpage.png" alt="Create HDFS MapReduce Wordcount finish web"></p>
<p>　　在此頁面右邊有個欄位Tracking UI有History可以連結，點進去後可以該job 的ID、名稱、花費的時間、每個Map與Reduce的log資訊與連結，如下圖所示，而連結裡更詳細的內容請您嘗試自己的Hadoop :P</p>
<p><img src="http://geekcodeparadise.com/wp-content/uploads/2015/09/JOBHISTORYLOG1.png" alt="Create HDFS MapReduce Wordcount web detail"></p>
<p>　　根據本人經驗，將Server重新開機後，8088 port的Job history都會被清掉，若想瀏覽之前job的歷史資訊，網址輸入hadoop01:19888 (依舊發發發?)，該網址是連到所有job history頁面，如下圖所示（圖片裡有我許多次提交job的紀錄）：</p>
<p><img src="http://geekcodeparadise.com/wp-content/uploads/2015/09/JOBHISTORYLOG2.png"></p>
<h5 id="2-4-查看Wordcount的輸出"><a href="#2-4-查看Wordcount的輸出" class="headerlink" title="2.4 查看Wordcount的輸出"></a>2.4 查看Wordcount的輸出</h5><p>　　到目前為止，得知job的運作紀錄該從哪瀏覽，但是最重要的wordcount結果在哪呢？就在HDFS的&#x2F;output01目錄下，此時用50070 port瀏覽，發現多了一個output01的目錄，如下圖：</p>
<p><img src="http://geekcodeparadise.com/wp-content/uploads/2015/09/hdfs2Boutput.png"></p>
<p>　　點進去看，會有兩個檔案，檔名分別為_SUCCESS與part-r-00000，其中_SUCCESS是一個記號，代表該次的檔案有寫入成功，而part-r-00000則是wordcount把詞彙計算完的輸出檔，如下圖：</p>
<p><img src="http://geekcodeparadise.com/wp-content/uploads/2015/09/hdfs2Boutput2B02.png"></p>
<p>　　把part-r-00000下載並打開來看，其內容就是詞彙的統計數量（本人總共有1588筆）：</p>
<p><img src="http://geekcodeparadise.com/wp-content/uploads/2015/09/wordcount2Bresult.png"></p>
<h4 id="3-結論"><a href="#3-結論" class="headerlink" title="3.結論"></a>3.結論</h4><p>　　架設好Hadoop後，確定能執行MapReduce的程式，代表最基本的Hadoop系統是可以work的。之後會寫一篇如何用Eclipse撰寫MapReduce Java程式。</p>
<h4 id="參考文獻"><a href="#參考文獻" class="headerlink" title="參考文獻"></a>參考文獻</h4><ol>
<li><a target="_blank" rel="noopener" href="http://stackoverflow.com/questions/25964054/yarn-does-not-show-the-finished-job">StackOverflow：YARN does not show the finished job</a></li>
<li><a target="_blank" rel="noopener" href="http://hadoop.apache.org/docs/r2.7.1/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml">官方mapred-site的預設參數</a></li>
<li><a target="_blank" rel="noopener" href="http://hadoop.apache.org/docs/r2.7.1/hadoop-yarn/hadoop-yarn-common/yarn-default.xml">官方yarn-site的預設參數</a> (這些預設參數都在<a target="_blank" rel="noopener" href="http://hadoop.apache.org/docs/r2.7.1/">官方頁面</a>的左下角可連結，對於要深入設置Hadoop系統很有幫助)</li>
</ol>
<h4 id="新增-修改日記"><a href="#新增-修改日記" class="headerlink" title="新增&#x2F;修改日記"></a>新增&#x2F;修改日記</h4><p>2015&#x2F;9&#x2F;29：</p>
<ol>
<li>移除mapred-site.xml的設置，全都採取預設的值才會正常運作。之前每個node都設定hadoop01:19888與hadoop01:10020的address，會使hadoop02與hadoop03因為port被hadoop01占住而無法運作，過一段時間會自己關掉jobhistoryserver。</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2015/09/06/hadoop-2-7-1-cluster/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiJyu Gao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GeekCodeParadise">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2015/09/06/hadoop-2-7-1-cluster/" class="post-title-link" itemprop="url">Hadoop 2.7.1 Cluster 安裝在 VirtualBox</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2015-09-06 23:05:00" itemprop="dateCreated datePublished" datetime="2015-09-06T23:05:00+08:00">2015-09-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-11-19 15:47:25" itemprop="dateModified" datetime="2023-11-19T15:47:25+08:00">2023-11-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Hadoop/" itemprop="url" rel="index"><span itemprop="name">Hadoop</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Big-Data/" itemprop="url" rel="index"><span itemprop="name">Big Data</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>安裝 Hadoop 2.7.1 Cluster 在 VirtualBox</p>
<h4 id="0-前言"><a href="#0-前言" class="headerlink" title="0.前言"></a>0.前言</h4><p>　　小弟開始在軟體業工作，是做Big Data Hadoop系統開發，開始會寫些對Hadoop系統或工作上學到的技術的文章，希望能幫助到一起學習的同伴。</p>
<p>　　首先本篇的內容是使用Oracle Virtual Box 安裝 Hadoop cluster，以下會依序介紹該如何安裝server、設定網路、安裝Hadoop、參考文獻等。<br>　　先非常簡單講解Hadoop的理論，如下圖所示</p>
<p><a target="_blank" rel="noopener" href="http://geekcodeparadise.com/wp-content/uploads/2015/09/hadoop-administration-pdf-21-638-1.jpg"><img src="http://geekcodeparadise.com/wp-content/uploads/2015/09/hadoop-administration-pdf-21-638.jpg" alt="Hadoop的理論"></a></p>
<p>來自於Edureka!：Hadoop Administration pdf</p>
<p> 　　Hadoop主要有HDFS(Hadoop Distributed File System)與MapReduce 2項核心系統。HDFS是分散式的檔案系統，任何要用Hadoop做資料運算，都得從HDFS存取。細分有namenode、secondarynamnenode及datanode，主節點(Master)的namenode管理datanode metadata(位置、大小等屬性)，secondarynamnenode是輔助namenode，分擔namenode的運作，而從節點(Slave)datanode則是存取資料的節點;MapReduce則是分散式的計算技術，主節點(Master)透過Job tracker會呼叫每個從節點的Task tracker做計算，從節點(Slave)Task tracker使用Map(Divide)函式將資料切割計算，之後各Task tracker使用Reduce(Conquer)函式結合計算結果並傳回到主節點。<br>　　本篇虛擬cluster共設置3台節點，第1台hadoop01為主節點(Master)，第2、3台hadoop02與hadoop03為從節點(Slaves)</p>
<h4 id="1-準備軟體工具與配備需求"><a href="#1-準備軟體工具與配備需求" class="headerlink" title="1.準備軟體工具與配備需求"></a>1.準備軟體工具與配備需求</h4><p>安裝Hadoop需要的軟體工具清單：</p>
<ul>
<li><p>(1) Oracle Virtual Box ，本篇使用5.0.4版，用此虛擬機來安裝server ， <a target="_blank" rel="noopener" href="https://www.virtualbox.org/wiki/Downloads">官方連結點我下載</a></p>
</li>
<li><p>(2) Linux 映像檔 ，本篇使用 Ubuntu 14.04.3 LTS 64位元server版本做為虛擬機的server，<a target="_blank" rel="noopener" href="http://www.ubuntu.com/download/server">官方連結點我下載</a></p>
</li>
<li><p>(3) Putty ，很常見的SSH&#x2F;Telnet 終端機連線工具，方便連server測試使用，<a target="_blank" rel="noopener" href="http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html">官方連結點我下載</a></p>
</li>
<li><p>(4) Notepad++ ，免費的文字編輯器，擁有FTP連線編輯文字檔的功能，不用在command mode 環境下用vim&#x2F;vi&#x2F;nano辛苦的編輯文字檔！另一篇文章會介紹如何使用FTP功能。<a target="_blank" rel="noopener" href="https://notepad-plus-plus.org/">官方連結點我下載</a></p>
</li>
<li><p>(5) FileZilla FTP client，可透過FTP上傳／下載server的資料。<a target="_blank" rel="noopener" href="https://filezilla-project.org/download.php?type=client">官方連結點我下載</a>  </p>
</li>
<li><p>(6) MobaXterm ，很多功能的SSH&#x2F;Telnet 終端機連線工具，其功能包含FTP、自動連結putty已設置的站台，畫面也好看，現在改用這個就能完成很多工作了。有免費版，<a target="_blank" rel="noopener" href="http://mobaxterm.mobatek.net/download.html">官方連結點我下載</a></p>
</li>
<li><p>(7) 瀏覽器，能打開網頁看Hadoop運行狀況（遭打）</p>
</li>
</ul>
<p>本篇主機含16GB的記憶體、2TB的硬碟，以下的虛擬機系統配置依照個人需求而調整。</p>
<h4 id="2-安裝Linux-Ubuntu-server虛擬機"><a href="#2-安裝Linux-Ubuntu-server虛擬機" class="headerlink" title="2.安裝Linux Ubuntu server虛擬機"></a>2.安裝Linux Ubuntu server虛擬機</h4><p>開啟Virtual Box，首先新增名字為hadoop01的Linux 64位元虛擬機，配置4GB的記憶體、100GB動態配置硬碟空間。設定好後，點右鍵設定值→網路，介面卡要設置兩個，第一個選擇用橋接介面卡，名稱選擇家裡網卡的driver，其功能是為了能連到外面網路，如下圖所示：</p>
<p><img src="http://geekcodeparadise.com/wp-content/uploads/2015/09/25E425BB258B25E9259D25A225E5258D25A11.png" alt="VirtualBox 橋接介面卡"></p>
<p>　　第二個選擇用「僅限主機」介面卡，是用來使本機與虛擬機互相連線，如下圖所示：</p>
<p><img src="http://geekcodeparadise.com/wp-content/uploads/2015/09/25E425BB258B25E9259D25A225E5258D25A12.png" alt="VirtualBox 橋接介面卡 2"></p>
<p>　　接著啟動虛擬機，選擇Ubuntu iso檔安裝。安裝過程重要的地方有5點：</p>
<ul>
<li>(1) 選英文的操作系統，用中文的話會有很高的機率發生路徑&#x2F;軟體不能安裝等問題…..</li>
<li>(2) hostname以本機為例是設定為hadoop01，作為主節點（Master），如下圖所示：</li>
</ul>
<p><img src="http://geekcodeparadise.com/wp-content/uploads/2015/09/25E825A825AD25E525AE259Ahostname.png" alt="hadoop01"></p>
<ul>
<li>(3) 安裝的套件選擇OpenSSH Server、LAMP Server與Samba file Server，OpenSSH是為了能建立安全連線，LAMP Server是包含Apache http server、MySQL與PHP語言，Samba file Server是可以主機間互相修改資料。其他的套件就依個人需求而安裝，如下圖所示，記得要先用空白鍵選擇再按continue</li>
</ul>
<p><img src="http://geekcodeparadise.com/wp-content/uploads/2015/09/25E525AE258925E825A3259DServer25E525A5259725E425BB25B6.png" alt="安裝的套件選擇OpenSSH Server、LAMP Server與Samba file Server"></p>
<ul>
<li>(4) 設定簡單點的Ubuntu登入id&#x2F;pwd，以本機的id&#x2F;pwd都設定為test，方便做測試。</li>
<li>(5) 硬碟的切割選擇用LVM</li>
<li>(6) 選擇安裝 security updates automaticall</li>
</ul>
<p>以上若都正常安裝的話，恭喜您能進入到黑黑的Command Mode登入畫面囉!</p>
<h4 id="3-線上安裝套件"><a href="#3-線上安裝套件" class="headerlink" title="3. 線上安裝套件"></a>3. 線上安裝套件</h4><p>　　接著要透過網路安裝server所需的軟體，首先執行下面指令，更新ubuntu可取得資源的server清單：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update</span><br></pre></td></tr></table></figure>

<p>　　以下會有2樣軟體要下載並安裝：</p>
<ul>
<li>(1) Oracle JAVA ：Hadoop執行是在JVM上，環境必須要有安裝JAVA。而目前以Hadoop文件最新測試的穩定JAVA版本是1.7，所以本機安裝1.7版本。安裝要下三行指令：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo add-apt-repository ppa:webupd8team/java</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install oracle-java7-installer</span><br></pre></td></tr></table></figure>

<p>　　其中用指令add-apt-repository ppa:webupd8team&#x2F;java原因 ：Ubuntu的預設資源清單是沒有Oracle的Java，必須透過Ubuntu另外設置的webupd8team網路空間來下載。</p>
<p>　　安裝完後的Java路徑預設是在&#x2F;usr&#x2F;lib&#x2F;jvm&#x2F;java-7-oracle</p>
<p>可以下指令檢查是否Java版本正確</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -version</span><br></pre></td></tr></table></figure>

<ul>
<li>(2) vsftpd：安裝Ubuntu的FTP Server，能夠方便傳檔案到Server，安裝要下此指令：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install vsftpd</span><br></pre></td></tr></table></figure>

<p>　　安裝完成後，修改vsftpd的設定檔</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vi /etc/vsftpd.conf</span><br></pre></td></tr></table></figure>

<p>　　有4項參數要改：</p>
<figure class="highlight apacheconf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#禁止無帳密登入</span></span><br><span class="line"></span><br><span class="line"><span class="attribute">anonymous_enable</span>=NO</span><br><span class="line"></span><br><span class="line"><span class="comment">#接受本地用戶登入</span></span><br><span class="line"></span><br><span class="line"><span class="attribute">local_enable</span>=YES</span><br><span class="line"></span><br><span class="line"><span class="comment">#允許上傳/寫入（預設沒打開，請把前面的#去掉就打開了)</span></span><br><span class="line"></span><br><span class="line"><span class="attribute">write_enable</span>=YES</span><br><span class="line"></span><br><span class="line"><span class="comment">#用戶只能訪問指定目錄</span></span><br><span class="line"></span><br><span class="line"><span class="attribute">chroot_local_user</span>=YES</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>　　設定完後，啟動服務vsftpd</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service vsftpd restart</span><br></pre></td></tr></table></figure>

<h4 id="4-修改網路設定"><a href="#4-修改網路設定" class="headerlink" title="4.修改網路設定"></a>4.修改網路設定</h4><p>　　為了配置每台的虛擬機網路IP，首先設定Virtual Box的網路設定，點選左上的檔案→喜好設定→網路，選取「僅限主機」網路的Virtual Box Host-Only Ethernet Adapter，將網路卡與DHCP伺服器的IP設定如下圖所示（其IP的值是由Virtual Box預設的，若想用別的IP也可行）：</p>
<p><img src="http://geekcodeparadise.com/wp-content/uploads/2015/09/25E725B625B225E825B725AF25E52596259C25E525A525BD25E825A825AD25E525AE259A1.png" alt="虛擬機網路IP"></p>
<p><img src="http://geekcodeparadise.com/wp-content/uploads/2015/09/25E725B625B225E825B725AF25E52596259C25E525A525BD25E825A825AD25E525AE259A2.png" alt="虛擬機網路IP 2"></p>
<p><img src="http://geekcodeparadise.com/wp-content/uploads/2015/09/25E725B625B225E825B725AF25E52596259C25E525A525BD25E825A825AD25E525AE259A3.png" alt="虛擬機網路IP 3"></p>
<p>　　設定完後，在hadoop01下指令更改網路IP設定：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vi /etc/network/interfaces</span><br></pre></td></tr></table></figure>

<p>　　在這檔案內容的下面增加這4行參數設定</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">auto eth1</span><br><span class="line">iface eth1 inet static</span><br><span class="line">address 192.168.56.101</span><br><span class="line">netmask 255.255.255.0</span><br></pre></td></tr></table></figure>

<p>其意思是我們用eth1的內部網路來連線溝通，而IP指定為192.168.56.101。</p>
<p>　　再來更改hosts，直接透過host做網路連線，不必每次都打IP。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vi /etc/hosts</span><br></pre></td></tr></table></figure>

<p>　　在此檔案內容上方的內容增加hadoop01 到 hadoop03的host設定，且要把IPv6的設定註解，否則用Hadoop的相關程式會有問題：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1 localhost</span><br><span class="line">192.168.56.101 hadoop01</span><br><span class="line">192.168.56.102 hadoop02</span><br><span class="line">192.168.56.103 hadoop03</span><br><span class="line"></span><br><span class="line"><span class="comment"># The following lines are desirable for IPv6 capable hosts</span></span><br><span class="line"><span class="comment">#::1 localhost ip6-localhost ip6-loopback</span></span><br><span class="line"><span class="comment">#ff02::1 ip6-allnodes</span></span><br><span class="line"><span class="comment">#ff02::2 ip6-allrouters</span></span><br></pre></td></tr></table></figure>

<p>以上設定完後，將Server重新啟動，再下指令查看是否IP有正確更改</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ip addr show</span><br></pre></td></tr></table></figure>

<p>　　視窗會顯示這樣的訊息：</p>
<p><img src="http://geekcodeparadise.com/wp-content/uploads/2015/09/IPADDRSHOW.png" alt="Hadoop 2.7.1 Cluster 檢查 IP"></p>
<p>　　可看見eth1網路卡有確實設定。  </p>
<p>　　接著設定ssh密鑰的設定，此設定是為了使Hadoop cluster之間連線可免密碼登入。下這兩行指令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 產生dsa的密鑰</span></span><br><span class="line">ssh-keygen -t dsa -P <span class="string">&#x27;&#x27;</span> -f ~/.ssh/id_dsa</span><br><span class="line"><span class="comment"># 將公鑰授權到key</span></span><br><span class="line">$ <span class="built_in">cat</span> ~/.ssh/id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure>

<p>　　試著對自己ssh連線：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh hadoop01</span><br></pre></td></tr></table></figure>

<p>會顯示是否連線(yes&#x2F;no) ，輸入yes，會顯示如下圖的訊息：</p>
<p><img src="http://geekcodeparadise.com/wp-content/uploads/2015/09/SSH25E8258725AA25E525B725B1.png" alt="Hadoop 2.7.1 Cluster SSH 連線"></p>
<h4 id="5-下載與安裝Hadoop-2-7-1"><a href="#5-下載與安裝Hadoop-2-7-1" class="headerlink" title="5.下載與安裝Hadoop 2.7.1"></a>5.下載與安裝Hadoop 2.7.1</h4><p>接著是安裝本篇主角－－－Hadoop！使用wget指令從<a target="_blank" rel="noopener" href="http://www.apache.org/dyn/closer.cgi/hadoop/common">Apache官方下載</a>：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo wget http://apache.stu.edu.tw/hadoop/common/hadoop-2.7.1/hadoop-2.7.1.tar.gz</span><br></pre></td></tr></table></figure>

<p>　　下載好後將其解壓縮</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo tar -zxvf hadoop-2.7.1.tar.gz</span><br></pre></td></tr></table></figure>

<p>　　更改Hadoop的設定，首先更改環境路徑的設定：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vi ~/.bashrc</span><br></pre></td></tr></table></figure>

<p>　　在最下方增加這些環境參數：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HADOOP_PREFIX=<span class="string">&quot;/home/test/hadoop-2.7.1&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=<span class="variable">$HADOOP_PREFIX</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> HADOOP_COMMON_HOME=<span class="variable">$HADOOP_PREFIX</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=<span class="variable">$HADOOP_PREFIX</span>/etc/hadoop</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> HADOOP_HDFS_HOME=<span class="variable">$HADOOP_PREFIX</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> HADOOP_MAPRED_HOME=<span class="variable">$HADOOP_PREFIX</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> HADOOP_YARN_HOME=<span class="variable">$HADOOP_PREFIX</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> HADOOP_COMMON_LIB_NATIVE_DIR=<span class="variable">$HADOOP_HOME</span>/lib/native</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> HADOOP_OPTS=<span class="string">&quot;-Djava.library.path=<span class="variable">$HADOOP_HOME</span>/lib&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/lib/jvm/java-7-oracle</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin:/home/test/hadoop-2.7.1/bin:/home/test/hadoop-2.7.1/sbin</span><br></pre></td></tr></table></figure>

<p>　　增加後存檔，再使系統重讀.bashrc檔</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> ~/.bashrc</span><br></pre></td></tr></table></figure>

<p>　　可以試著打下hadoop指令看看，會出現一些訊息</p>
<p><img src="http://geekcodeparadise.com/wp-content/uploads/2015/09/25E425B8258BHADOOP25E6258C258725E425BB25A4.png" alt="hadoop command"></p>
<p>代表有成功的設定環境變數。</p>
<p>　　將目錄切換到hadoop-2.7.1&#x2F;etc&#x2F;hadoop，須更改6項Hadoop的設定檔：core-site.xml、hdfs-site.xml(主節點mater與從節點slave有不一樣內容)、yarn-site.xml、mapred-site.xml、slaves(節點mater與從節點slave有不一樣內容)、hadoop-env.sh<br>　　hadoop01的設定(namenode master)<br>core-site.xml內容：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop01:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span> <span class="comment">&lt;!-- 將localhost改成hadoop01 host --&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>hdfs-site.xml內容：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/home/test/hdfs/datanode<span class="tag">&lt;/<span class="name">value</span>&gt;</span> <span class="comment">&lt;!-- hadoop hdfs(datanode)設定在此目錄 --&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/home/test/hdfs/namenode<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  <span class="comment">&lt;!-- hadoop hdfs(namenode)設定在此目錄 --&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span>  <span class="comment">&lt;!-- block 數量　--&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>yarn-site.xml內容：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services.mapreduce.shuffle.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span> org.apache.hadoop.mapred.ShuffleHandler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.resource-tracker.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span> <span class="comment">&lt;!-- 有localhost的都改為hadoop01 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop01:8025<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop01:8030<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop01:8050<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>原始的目錄只有mapred-site.xml.template檔案，需用cp複製一份改成mapred-site.xml：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cp</span> mapred-site.xml.template mapred-site.xml</span><br></pre></td></tr></table></figure>

<p>mapred-site.xml內容：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>slaves內容，把localhost那行去掉，改成主節點與從節點的host</p>
<figure class="highlight apacheconf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">hadoop01</span></span><br><span class="line"><span class="attribute">hadoop02</span></span><br><span class="line"><span class="attribute">hadoop03</span></span><br></pre></td></tr></table></figure>

<p>hadoop-env.sh內容，找到[export JAVA_HOME&#x3D;]這一行，將Java的路徑設置：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/lib/jvm/java-7-oracle</span><br></pre></td></tr></table></figure>

<p>以上六項檔案設定完後，在&#x2F;home&#x2F;test底下建立hdfs&#x2F;namenodem和datanode目錄：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#建立目錄</span></span><br><span class="line">sudo <span class="built_in">mkdir</span> -p ~/hdfs/namenode</span><br><span class="line">sudo <span class="built_in">mkdir</span> -p ~/hdfs/datanode</span><br><span class="line"><span class="comment">#更改權限為test使用者，之後啟動hdfs才能正常運作</span></span><br><span class="line">sudo <span class="built_in">chown</span> -R <span class="built_in">test</span>:<span class="built_in">test</span> ~/hdfs/namenode</span><br><span class="line">sudo <span class="built_in">chown</span> -R <span class="built_in">test</span>:<span class="built_in">test</span> ~/hdfs/datanode</span><br><span class="line">sudo <span class="built_in">chown</span> -R <span class="built_in">test</span>:<span class="built_in">test</span> ~</span><br></pre></td></tr></table></figure>

<p>　　以上6項更改完後，將hadoop01關閉，透過Virtual Box複製另外兩個虛擬機hadoop02與hadoop03。 對著hadoop01右鍵→再製，並勾選初始化所有網路卡的MAC位址：</p>
<p><img src="http://geekcodeparadise.com/wp-content/uploads/2015/09/clone.png" alt="複製虛擬機"></p>
<p>複製好這2台虛擬機，分別開啟更改些設定檔(hadoop02&#x2F;hadoop03: datanode slaves)</p>
<ul>
<li>hostname</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo vi /etc/hostname</span><br><span class="line"><span class="comment">#將hadoop01改成對應虛擬機的hadoop02(或hadoop03)</span></span><br></pre></td></tr></table></figure>

<ul>
<li>&#x2F;etc&#x2F;network&#x2F;interfaces</li>
</ul>
<figure class="highlight apacheconf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">auto</span> eth1</span><br><span class="line"><span class="attribute">iface</span> eth1 inet static</span><br><span class="line"><span class="attribute">address</span> <span class="number">192.168.56.102</span> #此IP為hadoop02的，hadoop03的設定改成<span class="number">192.168.56.103</span></span><br><span class="line"><span class="attribute">netmask</span> <span class="number">255.255.255.0</span></span><br></pre></td></tr></table></figure>

<ul>
<li>hadoop-2.7.1&#x2F;etc&#x2F;hadoop&#x2F;slaves內容清空，只增加localhost這一行</li>
</ul>
<figure class="highlight apacheconf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">localhost</span></span><br></pre></td></tr></table></figure>

<ul>
<li>hadoop-2.7.1&#x2F;etc&#x2F;hadoop&#x2F;hdfs-site.xml，內容與hadoop01不同，只有datanode：</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/home/test/hdfs/datanode<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>更改完hdfs-site.xml的內容後，將原本的hdfs&#x2F;namenode刪除，改成hdfs&#x2F;datanode：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo <span class="built_in">rm</span> -Rf ~/hdfs/namenode</span><br><span class="line"><span class="comment">#更改權限為test使用者，之後啟動hdfs才能正常運作</span></span><br><span class="line">sudo <span class="built_in">chown</span> -R <span class="built_in">test</span>:<span class="built_in">test</span> ~/hdfs/datanode</span><br><span class="line">sudo <span class="built_in">chown</span> -R <span class="built_in">test</span>:<span class="built_in">test</span> ~</span><br></pre></td></tr></table></figure>

<p>　　以上更改完後，三台虛擬機都重新啟動，先從hadoop01對hadoop02與hadoop03做ssh連線，確認都可以正常登入：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ssh hadoop02</span><br><span class="line">ssh hadoop03</span><br></pre></td></tr></table></figure>

<p>若能正常ssh登入後，在hadoo01這台下先下這項指令:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#此指令是將namenode初始化</span></span><br><span class="line">hadoop namenode -format</span><br></pre></td></tr></table></figure>

<p>接著再執行start-all.sh指令，這個shell會啟動hdfs與yarn services</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-all.sh</span><br></pre></td></tr></table></figure>

<p>　　會有下圖的啟動資訊，包含log檔的存檔位置：</p>
<p><img src="http://geekcodeparadise.com/wp-content/uploads/2015/09/START-ALL.png" alt="Hadoop 2.7.1 Cluster start-all.sh 啟動"></p>
<p>　　接著下jps指令，查看各台的Hadoop服務是否都正常起動：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jps</span><br></pre></td></tr></table></figure>

<p>以hadoop01(master)的狀態會是如下：</p>
<p><img src="http://geekcodeparadise.com/wp-content/uploads/2015/09/JPS-MASTER.png" alt="Hadoop 2.7.1 Cluster jps 狀態"></p>
<p>　　而hadoop02與hadoop03的狀態會是如下：</p>
<p><img src="http://geekcodeparadise.com/wp-content/uploads/2015/09/JPS-SLAVE.png" alt="Hadoop 2.7.1 Cluster jps 狀態 slave"></p>
<p>若您的三台的jps都跟本篇一樣，代表您成功安裝Hadoop了！</p>
<p><img src="http://geekcodeparadise.com/wp-content/uploads/2015/09/4055767_38d37c85866b5a95bcf9da64b82daf72.gif" alt="恭喜安裝完成"></p>
<p>　　接著在你的瀏覽器輸入網址: 192.168.56.101:50070 ，可以看見datanode的資訊</p>
<p><img src="http://geekcodeparadise.com/wp-content/uploads/2015/09/50070portpage.png" alt="Hadoop 2.7.1 Cluster 網頁"></p>
<p>而輸入網址:192.168.56.101:8088　，可看見job的資訊</p>
<p><img src="http://geekcodeparadise.com/wp-content/uploads/2015/09/8088portpage.png" alt="Hadoop 2.7.1 Cluster Job"></p>
<p>　　若要將Hadoop的service(hdfs與yarn)關閉，在hadoop01下此指令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stop-all.sh</span><br></pre></td></tr></table></figure>

<p>　　會顯示關閉的資訊，之後網頁就連不上囉：</p>
<p><img src="http://geekcodeparadise.com/wp-content/uploads/2015/09/STOP-ALL.png" alt="Hadoop 2.7.1 Cluster 斷線"></p>
<h4 id="6-結論"><a href="#6-結論" class="headerlink" title="6.結論"></a>6.結論</h4><p>　　完成這樣的安裝，才只是成功的一小步……本人在安裝的過程遇到很多問題，使用了不同版本的安裝方式就會出bug，只能說Hadoop變化真的很快！接著會陸續增加怎寫Hadoop的程式、安裝其他的套件等文章，而理論的部份我也正在學習，下面的參考文獻會列出一些翻過的書籍，可以從中了解Hadoop的運作，希望能幫到一起學習Hadoop的同伴，另外本文有錯誤或建議的部分請多指教，會做些修改。</p>
<h4 id="參考文獻"><a href="#參考文獻" class="headerlink" title="參考文獻"></a>參考文獻</h4><ol>
<li><a target="_blank" rel="noopener" href="http://hadoop.apache.org/docs/r2.7.1/">Hadoop 2.7.1官方文件</a></li>
<li><a target="_blank" rel="noopener" href="http://chaalpritam.blogspot.tw/2015/05/hadoop-270-multi-node-cluster-setup-on.html">chaalpritam blog：Hadoop 2.7.0 Multi Node Cluster Setup on Ubuntu 15.04</a>，有影片示範</li>
<li> <a target="_blank" rel="noopener" href="http://woodysclin.blogspot.tw/2014/05/hadoop-240-cluster.html">woodysclinblog：Hadoop 2.4.0 Cluster 多節點安裝紀錄</a> </li>
<li>書籍： <a target="_blank" rel="noopener" href="http://www.books.com.tw/products/0010633714">Hadoop實戰技術手冊(第2版)</a>，這本是中國的Hadoop聖經，有台灣出版社翻譯，內容似乎對中國原版做更正，使用Hadoop 2.3.0作範例</li>
<li>書籍：<a target="_blank" rel="noopener" href="https://www.amazon.com/-/zh_TW/Tom-White/dp/1449311520?crid=H3Z87CYB24CI&keywords=Hadoop:+The+Definitive+Guide&qid=1641613221&sprefix=hadoop+the+definitive+guide,aps,409&sr=8-1&linkCode=ll1&tag=glj89893320b-20&linkId=5dba868200c291617ea24b13999b1b49&language=zh_TW&ref_=as_li_ss_tl">Hadoop: The Definitive Guide</a><a target="_blank" rel="noopener" href="http://shop.oreilly.com/product/0636920033448.do">, 4th Edition</a> ，全球公認的Hadoop聖經，但我認為這本不適合新手…因為這本是講各種核心的運作理論，對於安裝的部分寫得很淺，會讓新手卡關在安裝步驟</li>
<li>書籍：<a target="_blank" rel="noopener" href="https://www.amazon.com/-/zh_TW/Garry-Turkington-ebook/dp/B00BKXQT8S?crid=1UAMK25KJE4SY&keywords=Hadoop+Beginner%27s+Guide&qid=1641613264&sprefix=hadoop+beginner%27s+guide,aps,235&sr=8-2&linkCode=ll1&tag=glj89893320b-20&linkId=762edc055277a4f7967c924a6d1e2373&language=zh_TW&ref_=as_li_ss_tl">Hadoop Beginner’s Guide</a>，這本我認為很適合新手，寫得很好懂，安裝步驟蠻清楚的</li>
</ol>
<h4 id="新增-修改日記"><a href="#新增-修改日記" class="headerlink" title="新增&#x2F;修改日記"></a>新增&#x2F;修改日記</h4><p>2015&#x2F;9&#x2F;20：</p>
<ol>
<li> 將Master hadoop01新增為datanode，更改了hadoop01的hdfs-site.xml、datanode目錄，是為了多個節點運算，之後要用來比較效能用。 </li>
<li>新增stop-all.sh 介紹</li>
</ol>
<p>2015&#x2F;9&#x2F;25：</p>
<ol>
<li>VirtualBox更新成5.0.4版，跟4.X版差不多功能，安裝Ubuntu和Hadoop都正常。</li>
</ol>
<p>2015&#x2F;9&#x2F;26：</p>
<ol>
<li>telnet&#x2F;ssh終端機連線工具新增MobaXterm，有免費版。</li>
</ol>
<p>2015&#x2F;10&#x2F;17：</p>
<ol>
<li>設置成hadoop01 至 hadoop05 五台虛擬機，01為Master，02~05為Slaves。</li>
</ol>
<p>2015&#x2F;11&#x2F;25：</p>
<ol>
<li>更改&#x2F;etc&#x2F;hosts的設定，要將IPv6的設定取消，有遇到使用HBase時zookeeper會connection refused</li>
</ol>
<p>Hadoop 2.7.1 Cluster</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2014/06/16/programming-contest-learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiJyu Gao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GeekCodeParadise">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2014/06/16/programming-contest-learning/" class="post-title-link" itemprop="url">程式解題的學習</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2014-06-16 16:33:00" itemprop="dateCreated datePublished" datetime="2014-06-16T16:33:00+08:00">2014-06-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-11-19 15:47:25" itemprop="dateModified" datetime="2023-11-19T15:47:25+08:00">2023-11-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ACM/" itemprop="url" rel="index"><span itemprop="name">ACM</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A8%8B%E5%BC%8F%E8%A7%A3%E9%A1%8C/" itemprop="url" rel="index"><span itemprop="name">程式解題</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>程式解題的學習</p>
<p><img src="https://geekcodeparadise.com/wp-content/uploads/2022/05/programming-g30ebf4378_1920-1024x683.jpg" alt="程式解題"></p>
<p>Photo from <a target="_blank" rel="noopener" href="https://pixabay.com/photos/programming-developing-startup-593312/">https://pixabay.com/photos/programming-developing-startup-593312/</a></p>
<p>真的畢業了! 找到適當的BLOG可以寫東西啦~~~就是GOOGLE的BLOGGER!有好多Plugin可以用、template可以自己設定, 在沒有網頁空間下, 覺得這蠻不錯的.</p>
<p>此篇是參考NPSC補完計畫，結合其內容與我的種種解題的學習過程與方法～</p>
<p>身邊的同學或學弟妹常會問如何學程式、提高程式設計能力或者如何解題。</p>
<p>　　我不是像中央MORRIS、清大楊易霖、台大以及其他各種怪物等國手，<br>但是在這多年來解題的過程有許多說不完的心得與想法，<br>在此分享給想挑戰程式設計比賽的人，<br>當作一個學習途徑的參考。</p>
<p>　　以下內容是以程式解題方向為主的學習過程與方法，無法涵蓋所有程式設計的學習方法，但我想共通點是一樣的。</p>
<h2 id="先熟練C-C-或JAVA基本語法"><a href="#先熟練C-C-或JAVA基本語法" class="headerlink" title="先熟練C&#x2F;C++或JAVA基本語法"></a>先熟練C&#x2F;C++或JAVA基本語法</h2><p>　　找一本你覺得看起來最舒服的書，<br>不要說去找什麼很有名的聖經本之類的，<br>除非底子已經很好，可以再來研讀有深度的書。</p>
<p>最最最最最基本的是需要以下（很重要所以強調）：</p>
<p>懂標準輸入輸出、變數宣告、<br>if-else、switch、while(do-while)、for等流程控制語法、<br>陣列、字串、struct的使用（C&#x2F;C++）、class的使用(JAVA、c++）<br>以及簡單的排序法。<br>進階一點就再學自定函數、遞迴函數等，<br>這階段主要是學習程式的語法，<br>讓你的想法可以很快地化成程式碼，<br>不用馬上說想寫很困難的程式。</p>
<h2 id="進入線上程式解題系統練習"><a href="#進入線上程式解題系統練習" class="headerlink" title="進入線上程式解題系統練習"></a>進入線上程式解題系統練習</h2><p>學程式絕對不只是要用”看”的就能學會，<br>學程式絕對不只是要用”看”的就能學會，<br>學程式絕對不只是要用”看”的就能學會，<br>學程式絕對不只是要用”看”的就能學會，<br>學程式絕對不只是要用”看”的就能學會，</p>
<p>（很重要所以我寫五行）<br>　　有些重要課程只有講理論而沒實際操刀寫程式，實在是無法深刻了解其中的奧妙而無法成長，特別是遇到需要 debug 的技巧是書上不會教的！<br>因為沒有人會有那個耐心幫你檢查所有的程式是不是有 bug，<br>所以找一個可以自動 Judge 的系統是很重要的～</p>
<p>新手的話，推薦兩個平台</p>
<p>第一個：「高中生程式解題系統」<br><a target="_blank" rel="noopener" href="http://l.facebook.com/l.php?u=http://zerojudge.tw/&h=fAQHqUcHd&s=1">http://zerojudge.tw/</a><br>　　歷史非常悠久的台灣中文解題系統，是由高雄師範大學經營，裡面的題庫大都是中文，也是高中職學生常用的解題訓練平台，許多TOI、IOI國手大都在這起步。可惜題目並沒有做難易度分類，即使在基礎題庫的題目也有很難的，需要有人指導會比較清楚。</p>
<p>第二個：「ITSA的E-TUTOR」<br><a target="_blank" rel="noopener" href="http://l.facebook.com/l.php?u=http://e-tutor.itsa.org.tw/e-Tutor/&h=pAQFbewZS&s=1">http://e-tutor.itsa.org.tw/e-Tutor/</a><br>　　近年來新的中文解題系統，網站上有中英文的題目，每種題目又有分各類型的題目，<br>更是有做難易度的區隔，讓學解題的使用者有個清楚的方向。<br>可惜這平台缺點是常有題目的敘述不完整，比如輸入測資的範圍沒說明等，<br>會給使用者額外的困擾。</p>
<h2 id="開始學習資料結構、演算法"><a href="#開始學習資料結構、演算法" class="headerlink" title="開始學習資料結構、演算法"></a>開始學習資料結構、演算法</h2><p>　　基本題解到一個程度，會發現學的東西不夠用，<br>這時候就要進入下一個階段,<br>開始學習資料結構和演算法了。</p>
<p>　　這部分一樣是去書店找一本你看得順眼的書,<br>以我個人而言，在資料結構的部分，除了有原文聖經本<br><a target="_blank" rel="noopener" href="https://amzn.to/3TMi9Sv">Ellis Horowitz寫的Fundamentals of Data Structures in C(或C++)</a><br>我有另外買<a target="_blank" rel="noopener" href="http://bit.ly/3JJX6va">蔡明志的「資料結構–使用C++(也有C++、JAVA版)」</a><br>另外有網友推薦<a target="_blank" rel="noopener" href="http://bit.ly/3z8PNZe">胡昭民的「圖解資料結構」</a><br>選一本即可，主要是看懂資料結構的觀念,<br>書裡程式碼希望是能研讀，最好是實作。<br>而有些樹的章節裡面, 只要看到二元樹(包括二元搜尋樹、Heap)就夠了,<br>後面AVL-Tree、2-3-4 Tree、B-Tree等可以不用看，基本上解題不會用到。</p>
<p>　　演算法的書, 之前我上課原文書是用　<a target="_blank" rel="noopener" href="https://amzn.to/3lAiGdK">Anany V. Levitin寫的Introduction to the Design &amp; Analysis of Algorithms</a>，而中文書推<a target="_blank" rel="noopener" href="http://bit.ly/3K6hyrB">薦蔡宗翰的「演算法：使用C++虛擬碼」</a>，這本的內容讓我學到很多。解題的程式最常會用的演算法包含各個擊破法 （Divide-and-Conquer）、動態規劃(Dynamic Programming)、貪婪演算法(Greedy)、<br>回溯(Backtracking)、分支界限法(Branch-and-Bound)等，有時解題遇到的問題可以用很多種演算法解。<br>資料結構與演算法的書讀完後，還不足的可以到「演算法筆記」挖資料<a target="_blank" rel="noopener" href="http://l.facebook.com/l.php?u=http://www.csie.ntnu.edu.tw/~u91029/&h=YAQF8w6u-&s=1">http://www.csie.ntnu.edu.tw/~u91029&#x2F;</a><br>這網站陪我好幾年，也有我幫站長debug文章的痕跡，可惜現在終止營運，有點可惜～<br>演算法聖經本是<a target="_blank" rel="noopener" href="https://amzn.to/40CxxmB">MIT教授Cormen寫的Introduction to Algorithms</a>，以台清交成資訊工程學生以及國手，幾乎都看這本學起，書的內容講的很完整，說是聖經本不為過，真的要讀可以啃這本，但不建議讀中文翻譯本，翻譯的很爛。</p>
<h2 id="使用進階的線上程式解題系統"><a href="#使用進階的線上程式解題系統" class="headerlink" title="使用進階的線上程式解題系統"></a>使用進階的線上程式解題系統</h2><p>　　第二段提到高中生程式解題系統(ZeroJudge)、E-TUTOR<br>雖然裡面部分題目比較簡單，可以很容易建立自信心、學習基本解題方法。<br>但到達某一程度後，有些題目會很難、甚至學習成長的幫助不大，此時可以改到其他網站練習。</p>
<p>　　首先最推薦的就是最多人用過， 俗稱 ACM 的 UVa Online Judge,<br><a target="_blank" rel="noopener" href="http://l.facebook.com/l.php?u=http://uva.onlinejudge.org/&h=UAQHC2g_d&s=1">http://uva.onlinejudge.org/</a><br>而「Lucky貓的ACM園地」有提供一些題目的中譯還有難易度分級,<br><a target="_blank" rel="noopener" href="http://l.facebook.com/l.php?u=http://luckycat.kshs.kh.edu.tw/&h=AAQE6XZVF&s=1">http://luckycat.kshs.kh.edu.tw/</a><br>可以搭配使用。當然是希望能看原文直接解題是最好，畢竟國際解題全都是用英文出題。</p>
<p>　　第 二個是Uhunt，這不是新的解題系統，而是UVa的實況系統，是由新加坡大學的解題團隊所建立的網站，提供現在全世界有哪些人在解題、解題狀況如何、排 名如何、程式執行時間多長，對我而言是個很刺激的網站，且長久下來會看見一些很奇怪的熟ID。而uhunt上中間有一個Competitive Programming Exercises，是新加坡解題團隊所分類的題目，有分好題目類型與難易度，也是提供使用者很完整的解題方向。</p>
<h2 id="進階解題書籍"><a href="#進階解題書籍" class="headerlink" title="進階解題書籍"></a>進階解題書籍</h2><p>　　前面提的資料結構與演算法書籍，只是＂基礎知識＂而已，有時學完後還是無法對某些題目想出方法，此時推薦幾本書籍。<br>第一是劉汝佳撰寫的「<a target="_blank" rel="noopener" href="https://amzn.to/40wjqz8">算法竞赛入门经典</a>」跟「<a target="_blank" rel="noopener" href="https://amzn.to/3K9fOh9">算法竞赛入门经典——训练指南</a>」<br>， 在台灣去年有代理出繁體版，名稱分別為《提升程式設計的邏輯思考力─國際程式設計競賽之演算法原理、題型、解題技巧與重點解析》與《提升程式設計的解題思 考力─國際演算法程式設計競賽訓練指南》。這兩本書而言，初學者先讀「算法竞赛入门经典」，讀完後再來讀「算法竞赛入门经典——训练指南」。書的內容就是 專門講解如何將基本的資料結構與演算法知識來解決程式解題的題目。以Uva而言，題目是變化多端，有些題目若沒有人指導確實很難自己想出來，而這些書提供 很多整合資料結構與演算法的概念，提升解題者的思路。<br>另外一題，劉汝佳起初是中國的解題國手，現任中國專業解題教練與ACM-ICPC命題委員，中國資訊學生幾乎都聽過他名字。他寫了這些書更是帶動中國的解題氣氛，每年上海交通大學幾乎都會進到ACM-ICPC決賽的前幾名，真的是很恐怖。</p>
<p>第二本是<a target="_blank" rel="noopener" href="https://amzn.to/3nhhxYN">日本國手寫的プログラミングコンテストチャレンジブック</a>，台灣也有代理成翻譯書，叫做《<a target="_blank" rel="noopener" href="http://bit.ly/42G1zaU">培養與鍛鍊程式設計的邏輯腦：世界級程式設計大賽的知識、心得與解題分享》</a>，也是跟劉汝佳寫得差不多，只是這本書是拿POJ跟GCJ的題目來講解。</p>
<h2 id="學海無涯"><a href="#學海無涯" class="headerlink" title="學海無涯"></a>學海無涯</h2><p>　　以解題而言，初學者最容易犯的錯，以為只看書就懂什麼叫做資料結構或演算法，甚至只挑簡單題來做，這樣是永遠不會成長。以我個人而言，大學的資料結構與演算 法都是有寫程式作業，且佔總成績很重，不寫好就是等著被當掉，因此有很多時間花在學好資料結構與演算法的理論與程式設計。從大二開始就跟著學長姐進入解題 比賽這一條不歸路（？），起初真的如同前面提到，即使課程學過資料結構與演算法的基礎知識，卻還是不會轉換成解題的方法，意思就是解題寫太少，且又碰得太 簡單，才沒有進步。之後到了大三，大概也才解了一百多題Uva題目，但覺得學的還是少。直到轉學至長榮，突然有個發神經的動力，開始瘋狂解題，真正體會到 資料結構與演算法的實作能力與奧妙之處，短短一年多解了４百題，這趟過程雖然辛苦，但是卻非常值得！<br>　　從以前去中山、成大的全國大專程式競 賽、南區程式競賽等比賽，常常看見很熟悉的成大選手臉孔，直到現在遇到的同一輩與新一代的強選手，這趟比賽過程值得回憶。最後今年的ITSA桂冠賽仍沒獲 獎，但也還是值得了，至少跟成大同解數還蠻高興(?)。而CPE(大學程式能力檢定)在上禮拜二拿下A+的成績，也是了無遺憾！<br>　　希望這篇文章有能幫助到有共同志趣在解題上的人，更希望能帶動程式解題氣氛，解題只有好處沒壞處，也如演算法筆記所述，解題能學到以下這五種能力：<br>　　一、智力思考：藉由智力測驗問題、益智遊戲（如數獨、孔明棋、倉庫番）、數學科普書等等，可以活絡大腦思路，培養觀察問題與分析問題的能力。<br>　　二、數學：從學校教科書可以學到很多數學概念、數學方法、甚至是數學公式，套用在問題上面來解決問題。<br>　　三、計算學：從學校教科書和網路上的資源，可以學到很多計算方法，套用在問題上面來解決問題。<br>　　四、程式語言：從程式語言的書籍（ <a target="_blank" rel="noopener" href="https://amzn.to/3z6swqz">C++ Primer</a> 、 <a target="_blank" rel="noopener" href="https://amzn.to/3zd3x4U">Effective C++</a> 、<a target="_blank" rel="noopener" href="http://bit.ly/3z5LC08">程式設計師的自我修養</a>）、計算機概論等書中學習。<br>　　五、程式設計：從 open source 與其他人寫的程式碼中學習一些寫程式的原則以及漂亮的寫法。<br>所以，解題Z&gt;B是百分百正確的!</p>
<h2 id="疑問"><a href="#疑問" class="headerlink" title="疑問"></a>疑問</h2><p>　　也有人會問，做解題那麼多對實際撰寫專題（系統）有用嗎？這要回答「部分有用」，畢竟一個系統就是要解決一大堆問題集合的工程方法，而解題只有解一部分小問 題。但是學過演算法會知道，要想辦法把問題切成子問題來解決，做系統也一樣，每一個系統可以切成各種功能的子系統，每一子系統是針對特定問題作解決的方 案。因此一個子系統的功能完整，正是一個（數個）程式的執行能力，而程式又如一位Niklaus Wirth大師所說：「程式　＝　演算法＋資料結構」。好得程式可以整合出一個好的系統，這是我個人對系統實作的觀念，所以解題學的好，仍然對做系統有幫 助。只是有些是解題學不到的，比如如何使用API來做完成一個子系統的功能，必須要有閱讀文件的能力，解題上沒有閱讀文件的學習方法。</p>
<p>-——- 2014&#x2F;6&#x2F;16補充———<br>　　這陣子接觸到2048 bot大賽，在這過程學到何謂對抗搜尋的branch-and-bound演算法，包含minimax、alpha-beta prune、expectimax、negascout等方法。目前ACM題目的解題我還沒遇到剪枝的題目，藉由這2048比賽的機會，學到了剪枝的精神！只是國內研究單位太強了，交大的居然能十幾%的16384 Tile….超好奇他們演算法能快狠準到此成績。</p>
<p>程式解題的學習</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/20/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/20/">20</a><span class="page-number current">21</span>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">LiJyu Gao</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">206</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">49</span>
        <span class="site-state-item-name">categories</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">LiJyu Gao</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

</body>
</html>
